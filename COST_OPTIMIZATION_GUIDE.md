# æ±‚èŒé¡¹ç›®æˆæœ¬ä¼˜åŒ–æŒ‡å— - é™è‡³æ¥è¿‘ $0/æœˆ

## ğŸ“Š å½“å‰æˆæœ¬ vs ä¼˜åŒ–åæˆæœ¬

| ç»„ä»¶ | å½“å‰æœˆæˆæœ¬ | ä¼˜åŒ–åæœˆæˆæœ¬ | èŠ‚çœ |
|------|-----------|-------------|------|
| AWS MWAA | $3,000-21,000 | **$0** (æœ¬åœ°Airflow) | 100% |
| Redshift Serverless | $30,000-99,000 | **$0** (PostgreSQL/DuckDB) | 100% |
| AWS Glue | $3,000-15,000 | **$0** (æœ¬åœ°PySpark/Pandas) | 100% |
| NAT Gateway | $32/æœˆ | **$0** (åˆ é™¤VPC) | 100% |
| KMS | $50-80 | **$0** (S3é»˜è®¤åŠ å¯†) | 100% |
| S3 | $50-200 | **$0.23-5** (ä»…å…è´¹å±‚) | 95%+ |
| CloudWatch | $20-50 | **$0** (åŸºç¡€å…è´¹) | 100% |
| Lambda | $0 (å…è´¹å±‚) | **$0** | - |
| Secrets Manager | $0.80 | **$0** (ç¯å¢ƒå˜é‡) | 100% |
| **æ€»è®¡** | **$36,000-135,000** | **< $5/æœˆ** | **99.9%** |

---

## ğŸš€ æ¶æ„æ”¹é€ æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: å®Œå…¨æœ¬åœ° + AWS S3 (æ¨èç”¨äºæ¼”ç¤º)
**æœˆæˆæœ¬: $0-1**

```
æœ¬åœ°å¼€å‘æœº
â”œâ”€â”€ Docker Compose
â”‚   â”œâ”€â”€ Airflow (standalone)
â”‚   â”œâ”€â”€ PostgreSQL (æ•°æ®ä»“åº“)
â”‚   â”œâ”€â”€ Grafana (ç›‘æ§)
â”‚   â””â”€â”€ MinIO (æœ¬åœ°S3æ¨¡æ‹Ÿ - å¯é€‰)
â””â”€â”€ Python Scripts
    â”œâ”€â”€ Pandas/DuckDB (æ›¿ä»£Glue)
    â””â”€â”€ APIè°ƒç”¨ (Alpha Vantageå…è´¹å±‚)

â†“ (å¯é€‰) ä¸Šä¼ ç»“æœåˆ° AWS S3 (å…è´¹å±‚: 5GB)
```

**ä¼˜åŠ¿:**
- âœ… å®Œå…¨å…è´¹è¿è¡Œ
- âœ… å¯ä»¥åœ¨ç¬”è®°æœ¬ä¸Šæ¼”ç¤º
- âœ… å¯åŠ¨å¿«é€Ÿ (docker-compose up)
- âœ… ä¿ç•™å®Œæ•´æŠ€æœ¯æ ˆå±•ç¤º
- âœ… å¯é€‰æ‹©æ€§ä¸Šä¼ åˆ°AWS S3å±•ç¤ºäº‘é›†æˆ

---

### æ–¹æ¡ˆ 2: AWS å…è´¹å±‚æ¶æ„
**æœˆæˆæœ¬: $0-5**

```
[Alpha Vantage API - å…è´¹å±‚]
        â†“
[EC2 t2.micro - å…è´¹å±‚ 750å°æ—¶/æœˆ]
  â”œâ”€â”€ Airflow Standalone
  â”œâ”€â”€ PostgreSQL
  â””â”€â”€ Python ETLè„šæœ¬
        â†“
[S3 å…è´¹å±‚ - 5GBå­˜å‚¨, 20,000 GET, 2,000 PUT]
        â†“
[DuckDB on EC2 - æŸ¥è¯¢S3ç›´æ¥]
        â†“
[CloudWatch å…è´¹å±‚ - 5GBæ—¥å¿—]
```

**ä¼˜åŠ¿:**
- âœ… åœ¨çœŸå®AWSç¯å¢ƒè¿è¡Œ
- âœ… å‡ ä¹å…è´¹ (< $5/æœˆ)
- âœ… é€‚åˆé¢è¯•æ—¶å±•ç¤º
- âœ… å¯æ‰©å±•åˆ°ä»˜è´¹å±‚

---

### æ–¹æ¡ˆ 3: GitHub Actions + å…è´¹æœåŠ¡
**æœˆæˆæœ¬: $0**

```
[GitHub Actions - 2000åˆ†é’Ÿ/æœˆå…è´¹]
  â”œâ”€â”€ å®šæ—¶è§¦å‘ (cron)
  â”œâ”€â”€ Pythonè„šæœ¬ (Pandaså¤„ç†)
  â””â”€â”€ å•å…ƒæµ‹è¯•
        â†“
[GitHub Releases / Artifacts]
  â””â”€â”€ å­˜å‚¨Parquetæ–‡ä»¶
        â†“
[DuckDBæŸ¥è¯¢æœ¬åœ°]
  â””â”€â”€ æˆ–ä¸Šä¼ åˆ° Kaggle Datasets (å…è´¹)
```

**ä¼˜åŠ¿:**
- âœ… 100% å…è´¹
- âœ… å±•ç¤ºCI/CDèƒ½åŠ›
- âœ… å…¬å¼€å¯è§ (å¼€æºé¡¹ç›®)
- âœ… è‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆ

---

## ğŸ› ï¸ è¯¦ç»†å®ç°æ–¹æ¡ˆ

### å®æ–½æ–¹æ¡ˆ 1: æœ¬åœ° Docker æ¶æ„ (æœ€æ¨è)

#### æ–°çš„ `docker-compose.yml`

```yaml
version: '3.8'

services:
  # PostgreSQL - æ›¿ä»£ Redshift
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: financial_dw
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

  # Airflow - æ›¿ä»£ AWS MWAA
  airflow:
    image: apache/airflow:2.8.1-python3.11
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/financial_dw
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      ALPHA_VANTAGE_API_KEY: ${ALPHA_VANTAGE_API_KEY}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-none}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-none}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./config:/opt/airflow/config
      - ./data:/opt/airflow/data  # æœ¬åœ°æ•°æ®å­˜å‚¨
      - airflow_logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db init &&
               airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
               airflow webserver & airflow scheduler"
    depends_on:
      - postgres

  # Grafana - ç›‘æ§å¯è§†åŒ– (æ›¿ä»£ CloudWatch Dashboard)
  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: grafana-postgresql-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    ports:
      - "3000:3000"
    depends_on:
      - postgres

  # MinIO - æœ¬åœ°S3æ›¿ä»£ (å¯é€‰)
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data
    ports:
      - "9000:9000"
      - "9001:9001"

volumes:
  postgres_data:
  airflow_logs:
  grafana_data:
  minio_data:
```

#### æˆæœ¬: **$0/æœˆ**

---

### æŠ€æœ¯æ ˆæ˜ å°„

| åŸAWSæœåŠ¡ | å…è´¹æ›¿ä»£ | åŠŸèƒ½ä¿ç•™ |
|----------|---------|---------|
| AWS MWAA | **Apache Airflow (Docker)** | âœ… 100% |
| Redshift Serverless | **PostgreSQL 15** | âœ… 95% (æ— å¤§è§„æ¨¡å¹¶è¡Œ) |
| AWS Glue | **Pandas + DuckDB** | âœ… 90% (å°æ•°æ®é›†) |
| S3 | **æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ / MinIO** | âœ… 100% (å¼€å‘) |
| KMS | **ä¸åŠ å¯† / GPG** | âš ï¸ (æ¼”ç¤ºç¯å¢ƒå¯æ¥å—) |
| CloudWatch | **Grafana + Loki** | âœ… 95% |
| Secrets Manager | **.env æ–‡ä»¶** | âš ï¸ (æ¼”ç¤ºç¯å¢ƒå¯æ¥å—) |
| Lambda | **Airflow PythonOperator** | âœ… 100% |
| NAT Gateway | **åˆ é™¤ (æœ¬åœ°æ— éœ€)** | N/A |

---

## ğŸ“ è¿ç§»æ­¥éª¤

### Step 1: åˆ›å»º Docker Compose ç¯å¢ƒ

```bash
# 1. åˆ›å»ºç›®å½•ç»“æ„
mkdir -p data/{raw,curated,validation}
mkdir -p sql monitoring/grafana

# 2. åˆ›å»º .env æ–‡ä»¶
cat > .env << 'EOF'
ALPHA_VANTAGE_API_KEY=your_free_api_key_here
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=financial_dw
EOF

# 3. å¯åŠ¨æœåŠ¡
docker-compose up -d

# 4. è®¿é—®æœåŠ¡
# Airflow UI: http://localhost:8080 (admin/admin)
# Grafana: http://localhost:3000 (admin/admin)
# MinIO: http://localhost:9001 (minioadmin/minioadmin)
```

---

### Step 2: ä¿®æ”¹ Python ETL ä»£ç 

**æ›¿æ¢ AWS Glue ä¸º Pandas/DuckDB:**

åˆ›å»º `src/transformation/local_transform.py`:

```python
import pandas as pd
import duckdb
from pathlib import Path

def transform_stock_data(input_path: str, output_path: str):
    """
    æœ¬åœ°æ›¿ä»£ AWS Glue PySpark ä½œä¸š
    ä½¿ç”¨ Pandas + DuckDB (é›¶æˆæœ¬)
    """
    # è¯»å– JSON (æ›¿ä»£ Glue çš„ S3 è¯»å–)
    df = pd.read_json(input_path)

    # æ•°æ®æ¸…æ´—
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.dropna(subset=['close', 'volume'])

    # æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
    df['sma_5'] = df.groupby('symbol')['close'].rolling(5).mean().reset_index(0, drop=True)
    df['sma_20'] = df.groupby('symbol')['close'].rolling(20).mean().reset_index(0, drop=True)
    df['daily_return'] = df.groupby('symbol')['close'].pct_change()

    # å†™å…¥ Parquet (æ›¿ä»£ Glue çš„ S3 å†™å…¥)
    df.to_parquet(output_path, compression='snappy', index=False)

    return len(df)

# DuckDB ç›´æ¥æŸ¥è¯¢ Parquet
def query_parquet(parquet_path: str):
    """
    DuckDB å¯ä»¥ç›´æ¥æŸ¥è¯¢ Parquet (æ›¿ä»£ Redshift)
    """
    conn = duckdb.connect(':memory:')
    result = conn.execute(f"""
        SELECT
            symbol,
            date_trunc('day', timestamp) as date,
            AVG(close) as avg_close,
            SUM(volume) as total_volume
        FROM read_parquet('{parquet_path}')
        GROUP BY symbol, date
        ORDER BY date DESC
    """).df()
    return result
```

**æ›´æ–° Airflow DAG:**

```python
# dags/financial_data_pipeline_local.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import sys
sys.path.insert(0, '/opt/airflow/src')

from transformation.local_transform import transform_stock_data
from ingestion.alpha_vantage_client import fetch_stock_data

with DAG(
    'financial_data_pipeline_local',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:

    fetch = PythonOperator(
        task_id='fetch_stock_data',
        python_callable=fetch_stock_data,
        op_kwargs={'symbols': ['AAPL', 'GOOGL']}
    )

    transform = PythonOperator(
        task_id='transform_data',
        python_callable=transform_stock_data,
        op_kwargs={
            'input_path': '/opt/airflow/data/raw/{{ ds }}.json',
            'output_path': '/opt/airflow/data/curated/{{ ds }}.parquet'
        }
    )

    fetch >> transform
```

---

### Step 3: æ•°æ®åº“è¿ç§» (Redshift â†’ PostgreSQL)

**åˆ›å»º `sql/init.sql`:**

```sql
-- åˆ›å»ºä¸ Redshift ç›¸åŒçš„è¡¨ç»“æ„
CREATE TABLE IF NOT EXISTS fact_stock_prices (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    open_price DECIMAL(18,4),
    high_price DECIMAL(18,4),
    low_price DECIMAL(18,4),
    close_price DECIMAL(18,4) NOT NULL,
    volume BIGINT,
    sma_5 DECIMAL(18,4),
    sma_20 DECIMAL(18,4),
    sma_50 DECIMAL(18,4),
    daily_return DECIMAL(10,6),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(symbol, timestamp)
);

CREATE INDEX idx_symbol_timestamp ON fact_stock_prices(symbol, timestamp DESC);

-- åˆ›å»ºç›‘æ§è¡¨ (æ›¿ä»£ CloudWatch Metrics)
CREATE TABLE IF NOT EXISTS pipeline_metrics (
    id SERIAL PRIMARY KEY,
    metric_name VARCHAR(100),
    metric_value DECIMAL(18,4),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**COPY æ›¿ä»£æ–¹æ¡ˆ:**

```python
# src/loading/postgres_loader.py
import psycopg2
import pandas as pd

def load_to_postgres(parquet_path: str):
    """
    æ›¿ä»£ Redshift COPY å‘½ä»¤
    """
    df = pd.read_parquet(parquet_path)

    conn = psycopg2.connect(
        host='postgres',
        database='financial_dw',
        user='airflow',
        password='airflow'
    )

    # æ‰¹é‡æ’å…¥ (ä½¿ç”¨ ON CONFLICT å¤„ç†é‡å¤)
    with conn.cursor() as cur:
        for _, row in df.iterrows():
            cur.execute("""
                INSERT INTO fact_stock_prices
                (symbol, timestamp, close_price, volume, sma_5, sma_20, daily_return)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (symbol, timestamp) DO NOTHING
            """, (row['symbol'], row['timestamp'], row['close'],
                  row['volume'], row['sma_5'], row['sma_20'], row['daily_return']))

    conn.commit()
    conn.close()
```

---

### Step 4: ç›‘æ§æ›¿ä»£æ–¹æ¡ˆ

**Grafana ä»ªè¡¨æ¿é…ç½®:**

åˆ›å»º `monitoring/grafana/dashboards/pipeline.json`:

```json
{
  "dashboard": {
    "title": "Financial Data Pipeline",
    "panels": [
      {
        "title": "Daily Records Ingested",
        "targets": [{
          "rawSql": "SELECT date_trunc('day', created_at) as time, COUNT(*) FROM fact_stock_prices GROUP BY 1 ORDER BY 1"
        }]
      },
      {
        "title": "Pipeline Success Rate",
        "targets": [{
          "rawSql": "SELECT metric_name, AVG(metric_value) FROM pipeline_metrics WHERE metric_name='success_rate' GROUP BY 1"
        }]
      }
    ]
  }
}
```

**Slack é€šçŸ¥æ›¿ä»£:**

```python
# ä½¿ç”¨ Airflow Email æˆ–ç®€å•çš„ HTTP è¯·æ±‚
from airflow.operators.python import PythonOperator
import requests

def send_notification(context):
    message = f"Pipeline {context['dag'].dag_id} completed"
    # å¯é€‰: ä½¿ç”¨å…è´¹çš„ Slack incoming webhook
    # æˆ–è€…ä»…æ‰“å°åˆ°æ—¥å¿—
    print(message)
```

---

## ğŸ¯ Alpha Vantage API å…è´¹å±‚é™åˆ¶

**å…è´¹è®¡åˆ’:**
- âœ… 5 è¯·æ±‚/åˆ†é’Ÿ
- âœ… 500 è¯·æ±‚/å¤©
- âœ… æ‰€æœ‰ç«¯ç‚¹è®¿é—®

**åº”å¯¹ç­–ç•¥:**
```python
# src/ingestion/alpha_vantage_client.py
import time

def fetch_with_rate_limit(symbols):
    """
    ç¬¦åˆå…è´¹å±‚é™åˆ¶: 5 req/min
    """
    for symbol in symbols:
        data = fetch_stock_data(symbol)
        time.sleep(12)  # 12ç§’ = 5æ¬¡/åˆ†é’Ÿ
        yield data
```

**å»ºè®®:**
- åªè·Ÿè¸ª 3-5 ä¸ªè‚¡ç¥¨ç¬¦å·
- æ¯å¤©è¿è¡Œ 1 æ¬¡ (è€Œéå®æ—¶)
- ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è¯·æ±‚

---

## ğŸ’° å®é™…æˆæœ¬åˆ†æ

### å®Œå…¨æœ¬åœ°æ–¹æ¡ˆ (Docker)

| èµ„æº | æˆæœ¬ |
|------|------|
| è®¡ç®— (æœ¬åœ°ç¬”è®°æœ¬) | $0 (å·²æœ‰è®¾å¤‡) |
| å­˜å‚¨ (1GBæ•°æ®) | $0 (æœ¬åœ°ç£ç›˜) |
| ç½‘ç»œ | $0 (å°‘é‡APIè°ƒç”¨) |
| **æœˆåº¦æ€»è®¡** | **$0** |

**é¢å¤–ç¡¬ä»¶éœ€æ±‚:**
- æœ€ä½: 4GB RAM, 20GB ç£ç›˜
- æ¨è: 8GB RAM, 50GB ç£ç›˜

---

### AWS å…è´¹å±‚æ–¹æ¡ˆ (å¯é€‰)

| æœåŠ¡ | å…è´¹å±‚ | é¢„è®¡ä½¿ç”¨ | è¶…é¢æˆæœ¬ |
|------|-------|---------|---------|
| EC2 t2.micro | 750å°æ—¶/æœˆ | 730å°æ—¶ | $0 |
| S3 | 5GB, 20K GET, 2K PUT | 2GB, 10K GET, 500 PUT | $0 |
| CloudWatch | 5GB æ—¥å¿— | 1GB | $0 |
| æ•°æ®ä¼ è¾“ | 100GB å‡ºç«™ | 5GB | $0 |
| **æœˆåº¦æ€»è®¡** | | | **$0-2** |

**æ³¨æ„:** å…è´¹å±‚æœ‰æ•ˆæœŸ **12ä¸ªæœˆ** (ä»æ³¨å†ŒAWSè´¦å·èµ·)

---

## ğŸ“Š åŠŸèƒ½å¯¹æ¯”çŸ©é˜µ

| åŠŸèƒ½ | AWSç”Ÿäº§ç‰ˆ | æœ¬åœ°ç‰ˆ | ä¿ç•™åº¦ |
|------|----------|-------|-------|
| å·¥ä½œæµç¼–æ’ | MWAA | Airflow Docker | âœ… 100% |
| æ•°æ®è´¨é‡ | Great Expectations | Great Expectations | âœ… 100% |
| ETLå¤„ç† | Glue PySpark | Pandas/DuckDB | âœ… 90% |
| æ•°æ®ä»“åº“ | Redshift | PostgreSQL | âœ… 85% |
| å¯¹è±¡å­˜å‚¨ | S3 | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | âœ… 95% |
| ç›‘æ§ | CloudWatch | Grafana | âœ… 90% |
| å‘Šè­¦é€šçŸ¥ | Lambda â†’ Slack | Airflow Email | âœ… 80% |
| åŠ å¯† | KMS | æ— /GPG | âš ï¸ 50% |
| å¯æ‰©å±•æ€§ | é«˜ | ä½ | âš ï¸ 30% |
| **æ€»ä½“æŠ€èƒ½å±•ç¤º** | | | **âœ… 90%+** |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä¸€é”®å¯åŠ¨æœ¬åœ°ç¯å¢ƒ

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo>
cd aws-mwaa-financial-data-platform

# åˆ›å»ºå…è´¹ Alpha Vantage API Key
# è®¿é—®: https://www.alphavantage.co/support/#api-key
# æ·»åŠ åˆ° .env
echo "ALPHA_VANTAGE_API_KEY=YOUR_KEY" > .env

# å¯åŠ¨ Docker ç¯å¢ƒ
docker-compose up -d

# ç­‰å¾… 30ç§’ æœåŠ¡å¯åŠ¨
sleep 30

# è®¿é—® Airflow
open http://localhost:8080
# ç™»å½•: admin / admin

# è§¦å‘ DAG
curl -X POST http://localhost:8080/api/v1/dags/financial_data_pipeline_local/dagRuns \
  -u admin:admin \
  -H "Content-Type: application/json" \
  -d '{"conf":{}}'
```

### 2. éªŒè¯æ•°æ®

```bash
# è¿æ¥ PostgreSQL
docker exec -it postgres psql -U airflow -d financial_dw

# æŸ¥è¯¢æ•°æ®
SELECT symbol, COUNT(*) as record_count
FROM fact_stock_prices
GROUP BY symbol;

# æŸ¥çœ‹æœ€æ–°ä»·æ ¼
SELECT * FROM fact_stock_prices
ORDER BY timestamp DESC
LIMIT 10;
```

### 3. æŸ¥çœ‹ç›‘æ§

```bash
# Grafana
open http://localhost:3000
# ç™»å½•: admin / admin

# å¯¼å…¥ä»ªè¡¨æ¿
# ä½¿ç”¨ monitoring/grafana/dashboards/pipeline.json
```

---

## ğŸ“ é¢è¯•å±•ç¤ºç­–ç•¥

### æ¼”ç¤ºæµç¨‹ (5åˆ†é’Ÿ)

1. **å±•ç¤ºæ¶æ„å›¾** (1åˆ†é’Ÿ)
   - "è¿™æ˜¯ç”Ÿäº§çº§AWSæ¶æ„ï¼Œä½†æˆ‘åˆ›å»ºäº†æˆæœ¬ä¼˜åŒ–ç‰ˆæœ¬..."

2. **è¿è¡Œ Docker Compose** (30ç§’)
   ```bash
   docker-compose up -d
   ```

3. **è§¦å‘ Airflow DAG** (1åˆ†é’Ÿ)
   - æ‰“å¼€ Airflow UI
   - æ‰‹åŠ¨è§¦å‘ DAG
   - å±•ç¤ºä»»åŠ¡ä¾èµ–å›¾

4. **æŸ¥è¯¢æ•°æ®** (1åˆ†é’Ÿ)
   - PostgreSQL æŸ¥è¯¢æŠ€æœ¯æŒ‡æ ‡
   - DuckDB æŸ¥è¯¢ Parquet æ–‡ä»¶

5. **å±•ç¤ºç›‘æ§** (1åˆ†é’Ÿ)
   - Grafana ä»ªè¡¨æ¿
   - æ•°æ®è´¨é‡æŠ¥å‘Š

6. **ä»£ç è®²è§£** (1åˆ†é’Ÿ)
   - æ•°æ®éªŒè¯è§„åˆ™
   - ETL è½¬æ¢é€»è¾‘

### å…³é”®è¯æœ¯

> "ä¸ºäº†å±•ç¤ºæŠ€æœ¯èƒ½åŠ›è€Œä¸äº§ç”Ÿé«˜é¢è´¹ç”¨ï¼Œæˆ‘è®¾è®¡äº†ä¸¤ä¸ªç‰ˆæœ¬ï¼š"
>
> 1. **ç”Ÿäº§ç‰ˆ (AWS)**: ä½¿ç”¨ MWAAã€Glueã€Redshift - å±•ç¤ºä¼ä¸šçº§æ¶æ„è®¾è®¡èƒ½åŠ›
> 2. **æ¼”ç¤ºç‰ˆ (Docker)**: å®Œå…¨æœ¬åœ°è¿è¡Œ - å±•ç¤ºæˆæœ¬æ„è¯†å’Œæ¶æ„çµæ´»æ€§
>
> "æ¼”ç¤ºç‰ˆä¿ç•™äº† 90%+ çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œæœˆæˆæœ¬ä» $36K é™è‡³ $0ï¼ŒåŒæ—¶ï¼š
> - âœ… ä¿ç•™ç›¸åŒçš„ Airflow DAG é€»è¾‘
> - âœ… ç›¸åŒçš„æ•°æ®è´¨é‡éªŒè¯
> - âœ… ç›¸åŒçš„ ETL å¤„ç†æµç¨‹
> - âœ… å¯ä¸€é”®åˆ‡æ¢å› AWS"

---

## ğŸ“ æŠ€èƒ½å±•ç¤ºæ¸…å•

é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å¯ä»¥å±•ç¤º:

### æ•°æ®å·¥ç¨‹
- âœ… ETL ç®¡é“è®¾è®¡
- âœ… æ•°æ®è´¨é‡éªŒè¯ (Great Expectations)
- âœ… åˆ—å¼å­˜å‚¨ä¼˜åŒ– (Parquet)
- âœ… å¢é‡æ•°æ®å¤„ç†
- âœ… SQL æŸ¥è¯¢ä¼˜åŒ–

### äº‘æ¶æ„
- âœ… AWS æœåŠ¡é€‰å‹ (IaC: CloudFormation)
- âœ… æˆæœ¬ä¼˜åŒ–æ„è¯†
- âœ… å®‰å…¨æœ€ä½³å®è·µ (IAM, åŠ å¯†)
- âœ… ç›‘æ§å’Œå‘Šè­¦

### DevOps
- âœ… Docker / Docker Compose
- âœ… åŸºç¡€è®¾æ–½å³ä»£ç  (IaC)
- âœ… CI/CD (å¯é€‰: GitHub Actions)
- âœ… æ—¥å¿—ç®¡ç†

### Python å¼€å‘
- âœ… API å®¢æˆ·ç«¯ (é‡è¯•ã€é™æµ)
- âœ… Pandas / PySpark
- âœ… æµ‹è¯• (pytest)
- âœ… ä»£ç è´¨é‡ (black, flake8)

---

## ğŸ”„ å‡çº§è·¯å¾„

### å¦‚æœéœ€è¦å®é™…éƒ¨ç½² AWS (é¢è¯•å)

å¯ä»¥æä¾›ä¸€ä¸ª"é¢„ç®—æ„è¯†ç‰ˆ AWS é…ç½®":

```yaml
# infrastructure/cloudformation/budget-version.yaml
Parameters:
  Environment:
    Default: demo  # è€Œé production

Mappings:
  EnvironmentConfig:
    demo:
      MWAAClass: mw1.small
      MWAAMaxWorkers: 2       # ä» 10 é™è‡³ 2
      RedshiftRPU: 8          # ä» 32 é™è‡³ 8
      RedshiftAutoPause: 1800 # 30åˆ†é’Ÿæ— æ´»åŠ¨åæš‚åœ
      GlueWorkers: 2          # ä» 10 é™è‡³ 2
      GlueExecution: FLEX     # èŠ‚çœ 60%
```

**é¢„è®¡æœˆæˆæœ¬: $500-800** (ä»ç„¶æ˜‚è´µä½†å¯æ¥å—)

---

## ğŸ“š é™„åŠ èµ„æº

### é…ç½®æ–‡ä»¶ç”Ÿæˆ

æˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆ:
1. âœ… å®Œæ•´çš„ `docker-compose.yml`
2. âœ… ç®€åŒ–ç‰ˆ Airflow DAG
3. âœ… Pandas æ›¿ä»£ Glue è„šæœ¬
4. âœ… PostgreSQL åˆå§‹åŒ– SQL
5. âœ… Grafana ä»ªè¡¨æ¿ JSON
6. âœ… README æ¼”ç¤ºè¯´æ˜

### æ–‡æ¡£å»ºè®®

åœ¨é¡¹ç›® README ä¸­æ·»åŠ :

```markdown
## ğŸ’° æˆæœ¬è¯´æ˜

æœ¬é¡¹ç›®æä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å¼:

### ğŸ¢ ç”Ÿäº§æ¨¡å¼ (AWS)
- é€‚åˆ: ä¼ä¸šç¯å¢ƒï¼Œå¤§è§„æ¨¡æ•°æ®
- æˆæœ¬: $36,000-135,000/æœˆ
- æ¶æ„: è§ `infrastructure/cloudformation/main.yaml`

### ğŸ’» æ¼”ç¤ºæ¨¡å¼ (Docker)
- é€‚åˆ: æœ¬åœ°å¼€å‘ï¼Œæ±‚èŒå±•ç¤º
- æˆæœ¬: $0/æœˆ
- æ¶æ„: è§ `docker-compose.yml`

**æ¼”ç¤ºæ¨¡å¼æä¾› 90%+ åŠŸèƒ½ï¼Œé€‚åˆæŠ€æœ¯é¢è¯•å±•ç¤º**
```

---

## âœ… æ€»ç»“

### è¡ŒåŠ¨è®¡åˆ’

1. **ç«‹å³å¯åš:**
   - âœ… åˆ›å»º `docker-compose.yml` (æˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆ)
   - âœ… æ·»åŠ  `COST_OPTIMIZATION_GUIDE.md` (æœ¬æ–‡æ¡£)
   - âœ… ä¿®æ”¹ DAG æ”¯æŒæœ¬åœ°æ¨¡å¼
   - âœ… æ›´æ–° README è¯´æ˜ä¸¤ç§æ¨¡å¼

2. **å¯é€‰ (å¢å¼ºç®€å†):**
   - âœ… æ·»åŠ  GitHub Actions CI/CD
   - âœ… åˆ›å»º Jupyter Notebook æ•°æ®åˆ†æç¤ºä¾‹
   - âœ… å½•åˆ¶ 5 åˆ†é’Ÿæ¼”ç¤ºè§†é¢‘
   - âœ… éƒ¨ç½²é™æ€ç½‘ç«™å±•ç¤ºæ¶æ„å›¾

3. **é¿å…:**
   - âŒ éƒ¨ç½²å®Œæ•´ AWS ç‰ˆæœ¬ (é™¤éé¢è¯•å®˜è¦æ±‚)
   - âŒ é•¿æ—¶é—´è¿è¡Œ MWAA/Redshift
   - âŒ å¯ç”¨ä¸å¿…è¦çš„ AWS æœåŠ¡

### é¢è¯•æ—¶çš„å›ç­”æ¨¡æ¿

**é¢è¯•å®˜**: "è¿™ä¸ªé¡¹ç›®çš„ AWS æˆæœ¬æ˜¯å¤šå°‘?"

**ä½ **: "ç”Ÿäº§çº§éƒ¨ç½²çº¦ $3.6ä¸‡-13.5ä¸‡/æœˆã€‚ä½†ä½œä¸ºæ±‚èŒé¡¹ç›®ï¼Œæˆ‘è®¾è®¡äº†æœ¬åœ° Docker ç‰ˆæœ¬ï¼Œæˆæœ¬ä¸º $0ï¼ŒåŒæ—¶ä¿ç•™ 90%+ æ ¸å¿ƒåŠŸèƒ½ã€‚è¿™å±•ç¤ºäº†æˆ‘çš„æˆæœ¬æ„è¯†å’Œæ¶æ„çµæ´»æ€§ã€‚éœ€è¦çš„è¯æˆ‘å¯ä»¥åœ¨ 2åˆ†é’Ÿå†…æ¼”ç¤ºå®Œæ•´æ•°æ®ç®¡é“ã€‚"

---

éœ€è¦æˆ‘å¸®ä½ ç”Ÿæˆä»»ä½•é…ç½®æ–‡ä»¶å—? æˆ‘å¯ä»¥åˆ›å»º:
- [ ] docker-compose.yml (å®Œæ•´ç‰ˆ)
- [ ] ç®€åŒ–çš„ Airflow DAG
- [ ] Pandas ETL è„šæœ¬
- [ ] PostgreSQL åˆå§‹åŒ–è„šæœ¬
- [ ] Grafana ä»ªè¡¨æ¿é…ç½®
- [ ] ä¸€é”®å¯åŠ¨è„šæœ¬ (setup.sh)

å‘Šè¯‰æˆ‘ä½ éœ€è¦å“ªäº›! ğŸš€
