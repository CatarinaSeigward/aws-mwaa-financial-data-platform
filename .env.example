# Financial Data Platform - Environment Configuration
# ====================================================

# =============================================================================
# 数据源配置
# =============================================================================

# Alpha Vantage API Key (可选 - 仅在使用真实API时需要)
# 获取免费 API Key: https://www.alphavantage.co/support/#api-key
# 默认使用模拟数据生成器，无需 API Key
ALPHA_VANTAGE_API_KEY=

# 数据源选择 (simulated | api)
# simulated: 使用模拟数据生成器（推荐用于演示）
# api: 使用 Alpha Vantage API（需要 API Key）
DATA_SOURCE=simulated

# =============================================================================
# Spark 配置
# =============================================================================

# Spark Master URL
# 本地 Docker: spark://spark-master:7077
# 本地单机: local[*]
SPARK_MASTER_URL=spark://spark-master:7077

# Spark Application 配置
SPARK_DRIVER_MEMORY=1g
SPARK_EXECUTOR_MEMORY=2g
SPARK_EXECUTOR_CORES=2

# =============================================================================
# AWS 配置 (可选 - 仅在使用 AWS 服务时需要)
# =============================================================================

# 部署模式 (local | aws)
# local: 使用本地 Docker + 本地文件系统
# aws: 使用 AWS EC2 + S3
DEPLOYMENT_MODE=local

# AWS Credentials (如果使用 AWS S3 / Glue / Redshift)
# 在 EC2 上推荐使用 IAM Role，留空即可
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=us-east-1

# S3 Bucket Names (如果 DEPLOYMENT_MODE=aws)
# 格式: s3://bucket-name
S3_RAW_BUCKET=s3://financial-raw-YOUR_ACCOUNT_ID
S3_CURATED_BUCKET=s3://financial-processed-YOUR_ACCOUNT_ID
S3_VALIDATION_BUCKET=s3://financial-validation-YOUR_ACCOUNT_ID

# =============================================================================
# 数据库配置
# =============================================================================

# PostgreSQL (本地数据仓库)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=financial_dw
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow

# Redshift (AWS 数据仓库 - 可选)
REDSHIFT_HOST=
REDSHIFT_PORT=5439
REDSHIFT_DB=
REDSHIFT_USER=
REDSHIFT_PASSWORD=

# =============================================================================
# Airflow 配置
# =============================================================================

# Airflow 数据库连接
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/financial_dw

# Airflow Executor
AIRFLOW__CORE__EXECUTOR=LocalExecutor

# Airflow 示例 DAG
AIRFLOW__CORE__LOAD_EXAMPLES=false

# =============================================================================
# 监控和通知 (可选)
# =============================================================================

# Slack Webhook URL (用于任务通知)
SLACK_WEBHOOK_URL=

# Grafana 配置
GF_SECURITY_ADMIN_PASSWORD=admin

# =============================================================================
# 数据生成配置
# =============================================================================

# 模拟数据预设 (demo | small | medium | large)
DATA_GENERATOR_PRESET=demo

# 模拟数据市场条件 (normal | bullish | bearish | volatile | stable)
MARKET_CONDITION=normal

# 生成数据的天数
DATA_GENERATOR_DAYS=90

# 股票代码列表 (用空格分隔)
DATA_GENERATOR_SYMBOLS=AAPL GOOGL MSFT AMZN TSLA

# =============================================================================
# 开发配置
# =============================================================================

# 环境 (development | staging | production)
ENVIRONMENT=development

# 日志级别 (DEBUG | INFO | WARNING | ERROR)
LOG_LEVEL=INFO

# 调试模式
DEBUG=false

# =============================================================================
# 端口配置 (用于本地开发)
# =============================================================================

# Airflow Web UI
AIRFLOW_PORT=8080

# Spark Master UI
SPARK_MASTER_UI_PORT=8081

# PostgreSQL
POSTGRES_EXTERNAL_PORT=5432

# Grafana
GRAFANA_PORT=3000

# MinIO (本地 S3)
MINIO_PORT=9000
MINIO_CONSOLE_PORT=9001

# =============================================================================
# 安全配置
# =============================================================================

# 加密密钥 (生产环境必须更改)
SECRET_KEY=change-this-in-production

# JWT Secret (生产环境必须更改)
JWT_SECRET=change-this-in-production

# =============================================================================
# 性能优化
# =============================================================================

# Spark Shuffle 分区数
SPARK_SQL_SHUFFLE_PARTITIONS=8

# 启用 Spark 自适应查询执行
SPARK_SQL_ADAPTIVE_ENABLED=true

# Parquet 压缩格式 (snappy | gzip | lzo)
PARQUET_COMPRESSION=snappy

# =============================================================================
# 数据质量配置
# =============================================================================

# Great Expectations 验证失败阈值 (%)
DATA_QUALITY_FAILURE_THRESHOLD=5

# 是否在验证失败时停止管道
STOP_ON_VALIDATION_FAILURE=true

# =============================================================================
# 备份和保留
# =============================================================================

# 原始数据保留天数
RAW_DATA_RETENTION_DAYS=30

# 验证报告保留天数
VALIDATION_REPORT_RETENTION_DAYS=90

# Airflow 日志保留天数
AIRFLOW_LOG_RETENTION_DAYS=30

# =============================================================================
# 说明
# =============================================================================

# 1. 复制此文件为 .env:
#    cp .env.example .env
#
# 2. 根据您的环境修改配置
#
# 3. 对于求职演示，推荐配置：
#    - DATA_SOURCE=simulated (无需 API Key)
#    - DATA_GENERATOR_PRESET=demo
#    - SPARK_MASTER_URL=spark://spark-master:7077 (使用 Docker)
#
# 4. 对于生产环境：
#    - 设置 ALPHA_VANTAGE_API_KEY
#    - 配置 AWS 凭证
#    - 更改所有密码和密钥
#    - 使用 Secrets Manager 存储敏感信息
