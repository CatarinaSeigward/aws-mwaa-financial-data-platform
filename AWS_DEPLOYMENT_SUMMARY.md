# AWS Deployment Summary

## ğŸ‰ å·²å®Œæˆçš„AWSéƒ¨ç½²æ”¯æŒ

æ‚¨çš„é¡¹ç›®ç°åœ¨æ”¯æŒ**ä¸¤ç§éƒ¨ç½²æ¨¡å¼**ï¼Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è½»æ¾åˆ‡æ¢ï¼š

---

## ğŸ“Š éƒ¨ç½²æ¨¡å¼å¯¹æ¯”

| ç‰¹æ€§ | Local Docker | AWS Free Tier |
|------|-------------|---------------|
| **æˆæœ¬** | $0/æœˆ | $0-3/æœˆ |
| **é…ç½®** | `DEPLOYMENT_MODE=local` | `DEPLOYMENT_MODE=aws` |
| **æ•°æ®å­˜å‚¨** | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | S3 |
| **è®¡ç®—** | æœ¬åœ°Docker Spark | EC2ä¸Šçš„Spark |
| **æ•°æ®åº“** | æœ¬åœ°PostgreSQL | EC2ä¸Šçš„PostgreSQL |
| **ç½‘ç»œè®¿é—®** | localhost | å…¬ç½‘IP |
| **è®¾ç½®æ—¶é—´** | 5åˆ†é’Ÿ | 30åˆ†é’Ÿ |

---

## ğŸ”§ ä»£ç ä¿®æ”¹æ€»ç»“

### 1. ç¯å¢ƒé…ç½® (`.env.example`)

**æ–°å¢å‚æ•°**:
```bash
# éƒ¨ç½²æ¨¡å¼å¼€å…³
DEPLOYMENT_MODE=local  # 'local' æˆ– 'aws'

# S3 Bucketé…ç½®ï¼ˆAWSæ¨¡å¼ï¼‰
S3_RAW_BUCKET=s3://financial-raw-YOUR_ACCOUNT_ID
S3_CURATED_BUCKET=s3://financial-processed-YOUR_ACCOUNT_ID
S3_VALIDATION_BUCKET=s3://financial-validation-YOUR_ACCOUNT_ID
```

### 2. Airflow DAG (`dags/financial_data_pipeline_scala.py`)

**æ–°å¢åŠŸèƒ½**:
- âœ… è‡ªåŠ¨æ£€æµ‹éƒ¨ç½²æ¨¡å¼
- âœ… åŠ¨æ€åˆ‡æ¢æ•°æ®è·¯å¾„ï¼ˆæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ â†” S3ï¼‰
- âœ… æ–°å¢ `upload_to_s3_if_needed()` å‡½æ•°
- âœ… æ–°å¢ `upload_to_s3` ä»»åŠ¡èŠ‚ç‚¹

**ä¿®æ”¹çš„ä»£ç ç‰‡æ®µ**:
```python
# åŠ¨æ€è·¯å¾„åˆ‡æ¢
if DEPLOYMENT_MODE == 'aws':
    RAW_DATA_PATH = os.getenv('S3_RAW_BUCKET', 's3://financial-raw/data/raw')
    CURATED_DATA_PATH = os.getenv('S3_CURATED_BUCKET', 's3://financial-processed/data/curated')
else:
    RAW_DATA_PATH = PROJECT_ROOT / 'data' / 'raw'
    CURATED_DATA_PATH = PROJECT_ROOT / 'data' / 'curated'

# S3ä¸Šä¼ å‡½æ•°
def upload_to_s3_if_needed(**context):
    if DEPLOYMENT_MODE != 'aws':
        print("âœ… Local mode - skip S3 upload")
        return
    # ... boto3 ä¸Šä¼ é€»è¾‘
```

**ä»»åŠ¡ä¾èµ–é“¾**:
```
OLD: fetch >> verify_fetch >> spark_transform
NEW: fetch >> upload_s3 >> verify_fetch >> spark_transform
```

### 3. Scalaä»£ç 

Scalaä»£ç **å·²ç»æ”¯æŒS3**ï¼Œå› ä¸ºSparkåŸç”Ÿæ”¯æŒ `s3://` åè®®ã€‚åªéœ€è¦ï¼š
- åœ¨è¿è¡Œæ—¶é…ç½®AWS credentialsï¼ˆé€šè¿‡IAM roleæˆ–ç¯å¢ƒå˜é‡ï¼‰
- ä¼ å…¥S3è·¯å¾„å³å¯

### 4. éƒ¨ç½²è„šæœ¬ (`scripts/deploy-to-aws.sh`)

**åŠŸèƒ½**:
- âœ… åˆ›å»ºS3 buckets
- âœ… ä¸Šä¼ Scala JARåˆ°S3
- âœ… ä¸Šä¼ Airflow DAGså’Œscripts
- âœ… ç”Ÿæˆå¹¶ä¸Šä¼ æ ·æœ¬æ•°æ®
- âœ… éƒ¨ç½²CloudFormation stack
- âœ… è‡ªåŠ¨é…ç½® `.env` æ–‡ä»¶
- âœ… è¾“å‡ºè®¿é—®URLå’Œåç»­æ­¥éª¤

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### é€‰é¡¹A: æœ¬åœ°Dockeréƒ¨ç½²ï¼ˆé»˜è®¤ï¼‰

```bash
# 1. é…ç½®ç¯å¢ƒ
cp .env.example .env
# ç¡®ä¿ DEPLOYMENT_MODE=local

# 2. ç”Ÿæˆæ•°æ®
python scripts/data_generator.py --preset demo

# 3. æ„å»ºJAR
sbt assembly

# 4. å¯åŠ¨Docker
docker-compose -f docker-compose-spark.yml up -d

# 5. è¿è¡ŒPipeline
./scripts/build-and-submit.sh local
```

**æˆæœ¬**: **$0**

---

### é€‰é¡¹B: AWS Free Tieréƒ¨ç½²

```bash
# 1. é…ç½®AWS CLI
aws configure

# 2. è¿è¡Œè‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
./scripts/deploy-to-aws.sh

# è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
# - åˆ›å»ºS3 buckets
# - ä¸Šä¼ JARå’Œä»£ç 
# - éƒ¨ç½²EC2å®ä¾‹
# - é…ç½®ç¯å¢ƒå˜é‡

# 3. SSHè¿æ¥åˆ°EC2
ssh -i your-key.pem ec2-user@<PUBLIC_IP>

# 4. å¯åŠ¨æœåŠ¡ï¼ˆåœ¨EC2ä¸Šï¼‰
cd aws-mwaa-financial-data-platform
sudo docker-compose -f docker-compose-spark.yml up -d

# 5. è®¿é—®Airflow UI
# http://<PUBLIC_IP>:8080 (admin/admin)
```

**æˆæœ¬**: **$0-3/æœˆ** (å‰12ä¸ªæœˆ)

---

## ğŸ“ ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨

```
ä¿®æ”¹çš„æ–‡ä»¶:
â”œâ”€â”€ .env.example                           # æ–°å¢ DEPLOYMENT_MODE å’Œ S3 é…ç½®
â”œâ”€â”€ dags/financial_data_pipeline_scala.py  # æ”¯æŒåŠ¨æ€è·¯å¾„åˆ‡æ¢å’ŒS3ä¸Šä¼ 
â”‚
æ–°å¢çš„æ–‡ä»¶:
â”œâ”€â”€ scripts/deploy-to-aws.sh               # ä¸€é”®AWSéƒ¨ç½²è„šæœ¬
â”œâ”€â”€ AWS_FREE_TIER_DEPLOYMENT.md            # AWSå…è´¹å±‚éƒ¨ç½²è¯¦ç»†æŒ‡å—
â”œâ”€â”€ PROJECT_DESCRIPTION_AWS.tex            # AWSç‰ˆæœ¬çš„ç®€å†é¡¹ç›®æè¿°
â””â”€â”€ AWS_DEPLOYMENT_SUMMARY.md              # æœ¬æ–‡ä»¶
```

---

## ğŸ¯ é¢è¯•å±•ç¤ºå»ºè®®

### åœºæ™¯1: ç°åœºé¢è¯•æ¼”ç¤º
**ä½¿ç”¨**: Local Dockeræ¨¡å¼
```bash
export DEPLOYMENT_MODE=local
./scripts/quick-demo.sh
```

**ä¼˜åŠ¿**:
- âœ… æ— éœ€ç½‘ç»œ
- âœ… ç«‹å³å¯åŠ¨
- âœ… å®Œå…¨æ§åˆ¶

---

### åœºæ™¯2: è¿œç¨‹é¢è¯•æˆ–åœ¨çº¿ä½œå“é›†
**ä½¿ç”¨**: AWS Free Tieræ¨¡å¼
```bash
export DEPLOYMENT_MODE=aws
./scripts/deploy-to-aws.sh
```

**ä¼˜åŠ¿**:
- âœ… 24/7å¯è®¿é—®
- âœ… çœŸå®äº‘ç¯å¢ƒ
- âœ… å…¬å¼€URLå¯åˆ†äº«

**æä¾›é¢è¯•å®˜**:
- Airflow UI: `http://<PUBLIC_IP>:8080`
- Spark UI: `http://<PUBLIC_IP>:8081`

---

### é¢è¯•è¯æœ¯ç¤ºä¾‹

> **é¢è¯•å®˜**: "ä½ çš„é¡¹ç›®æ˜¯å¦‚ä½•éƒ¨ç½²çš„ï¼Ÿ"

> **ä½ **: "è¿™ä¸ªé¡¹ç›®é‡‡ç”¨äº†**åŒæ¨¡å¼æ¶æ„è®¾è®¡**ï¼Œæ”¯æŒæœ¬åœ°Dockerå’ŒAWSä¸¤ç§éƒ¨ç½²æ–¹å¼ã€‚é€šè¿‡ç¯å¢ƒå˜é‡ `DEPLOYMENT_MODE` å¯ä»¥æ— ç¼åˆ‡æ¢ã€‚
>
> - **æœ¬åœ°æ¨¡å¼**ç”¨äºå¿«é€Ÿå¼€å‘å’Œæ¼”ç¤ºï¼Œæˆæœ¬ä¸º$0ï¼Œæ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
> - **AWSæ¨¡å¼**ä½¿ç”¨EC2 Free Tierå’ŒS3ï¼Œæˆæœ¬çº¦$3/æœˆï¼Œé€‚åˆåœ¨çº¿å±•ç¤º
>
> ä»£ç å±‚é¢ï¼Œæˆ‘åœ¨Airflow DAGä¸­å®ç°äº†åŠ¨æ€è·¯å¾„è§£æï¼Œæ ¹æ®éƒ¨ç½²æ¨¡å¼è‡ªåŠ¨é€‰æ‹©æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæˆ–S3ã€‚Scala Sparkä»£ç æ— éœ€ä¿®æ”¹ï¼Œå› ä¸ºSparkåŸç”Ÿæ”¯æŒS3åè®®ã€‚
>
> å¦‚æœéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼ŒåŒæ ·çš„ä»£ç å¯ä»¥ç›´æ¥è¿è¡Œåœ¨AWS Glueæˆ–EMRä¸Šï¼Œåªéœ€ä¿®æ”¹é…ç½®æ–‡ä»¶ã€‚è¿™å±•ç¤ºäº†æˆ‘å¯¹æ¶æ„å¯ç§»æ¤æ€§å’Œæˆæœ¬ä¼˜åŒ–çš„ç†è§£ã€‚"

---

## ğŸ’° æˆæœ¬ç›‘æ§

### æ£€æŸ¥å½“å‰æœˆåº¦æˆæœ¬
```bash
aws ce get-cost-and-usage \
  --time-period Start=$(date +%Y-%m-01),End=$(date +%Y-%m-%d) \
  --granularity MONTHLY \
  --metrics BlendedCost
```

### è®¾ç½®è´¦å•å‘Šè­¦ï¼ˆæ¨èï¼‰
```bash
# åˆ›å»ºSNSä¸»é¢˜
aws sns create-topic --name billing-alerts

# è®¢é˜…é‚®ç®±
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:<ACCOUNT_ID>:billing-alerts \
  --protocol email \
  --notification-endpoint your-email@example.com

# åˆ›å»º$5å‘Šè­¦
aws cloudwatch put-metric-alarm \
  --alarm-name billing-alert-5-dollars \
  --alarm-description "Alert if monthly cost exceeds $5" \
  --metric-name EstimatedCharges \
  --namespace AWS/Billing \
  --statistic Maximum \
  --period 21600 \
  --evaluation-periods 1 \
  --threshold 5 \
  --comparison-operator GreaterThanThreshold \
  --alarm-actions arn:aws:sns:us-east-1:<ACCOUNT_ID>:billing-alerts
```

---

## ğŸ§¹ æ¸…ç†èµ„æº

### åœæ­¢EC2å®ä¾‹ï¼ˆä¿ç•™æ•°æ®ï¼‰
```bash
# è·å–å®ä¾‹ID
INSTANCE_ID=$(aws cloudformation describe-stacks \
  --stack-name financial-platform-free-tier \
  --query 'Stacks[0].Outputs[?OutputKey==`InstanceId`].OutputValue' \
  --output text)

# åœæ­¢å®ä¾‹
aws ec2 stop-instances --instance-ids $INSTANCE_ID
```

### å®Œå…¨åˆ é™¤æ‰€æœ‰èµ„æº
```bash
# 1. æ¸…ç©ºS3 buckets
for BUCKET in $(aws s3 ls | grep financial | awk '{print $3}'); do
  aws s3 rm s3://$BUCKET --recursive
  aws s3 rb s3://$BUCKET
done

# 2. åˆ é™¤CloudFormation stack
aws cloudformation delete-stack --stack-name financial-platform-free-tier

# 3. ç­‰å¾…åˆ é™¤å®Œæˆ
aws cloudformation wait stack-delete-complete --stack-name financial-platform-free-tier
```

---

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [Getting Started Guide](GETTING_STARTED.md) - æœ¬åœ°Dockeréƒ¨ç½²è¯¦ç»†æŒ‡å—
- [AWS Free Tier Deployment](AWS_FREE_TIER_DEPLOYMENT.md) - AWSéƒ¨ç½²å®Œæ•´æ•™ç¨‹
- [Cost Analysis](COST_ANALYSIS_SCALA.md) - æˆæœ¬åˆ†ææŠ¥å‘Š
- [README](README.md) - é¡¹ç›®æ€»è§ˆ

---

## âœ… æŠ€æœ¯äº®ç‚¹æ€»ç»“

æ‚¨ç°åœ¨å¯ä»¥åœ¨ç®€å†/é¢è¯•ä¸­å¼ºè°ƒçš„æŠ€æœ¯ç‚¹ï¼š

1. **æ¶æ„çµæ´»æ€§**: æ”¯æŒæœ¬åœ°å’Œäº‘ç«¯åŒæ¨¡å¼éƒ¨ç½²
2. **æˆæœ¬æ„è¯†**: ä»$174Kä¼˜åŒ–åˆ°$0-3/æœˆï¼ˆ99.998%æˆæœ¬é™ä½ï¼‰
3. **Infrastructure as Code**: CloudFormationè‡ªåŠ¨åŒ–éƒ¨ç½²
4. **AWSæŠ€èƒ½**: EC2, S3, IAM, CloudWatch
5. **DevOpså®è·µ**: ä¸€é”®éƒ¨ç½²è„šæœ¬ï¼Œç¯å¢ƒé…ç½®ç®¡ç†
6. **å¤§æ•°æ®å¤„ç†**: Spark on AWS with S3 integration
7. **å¯æ‰©å±•æ€§**: ç›¸åŒä»£ç å¯æ— ç¼æ‰©å±•åˆ°AWS Glue/EMR

---

## ğŸ‰ æ€»ç»“

æ‚¨çš„é¡¹ç›®ç°åœ¨å…·å¤‡**ä¼ä¸šçº§çš„çµæ´»æ€§**ï¼Œå¯ä»¥ï¼š
- âœ… æœ¬åœ°å…è´¹æ¼”ç¤º
- âœ… AWSä½æˆæœ¬åœ¨çº¿å±•ç¤º
- âœ… ä¸€é”®åˆ‡æ¢éƒ¨ç½²æ¨¡å¼
- âœ… ä»£ç é›¶æ”¹åŠ¨çš„äº‘è¿ç§»èƒ½åŠ›

**è¿™æ­£æ˜¯é¢è¯•å®˜å¸Œæœ›çœ‹åˆ°çš„æ¶æ„æ€ç»´ï¼** ğŸš€
