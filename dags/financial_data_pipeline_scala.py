"""
Financial Data Pipeline DAG - Scala + Spark Version
====================================================
Airflow DAG that orchestrates Scala Spark ETL jobs with idempotent task design.

Idempotency guarantees:
- Every task is safe to re-run for the same execution_date without side effects
- Ingestion: cleans target partition before writing (delete-then-write)
- Transformation: Spark writes with SaveMode.Overwrite to execution-date partition
- Loading: UPSERT via DELETE+INSERT in a single transaction (keyed on price_id)
- All output paths are partitioned by execution_date ({{ ds }})

Features:
- Scala + Spark data transformation with typed Dataset API
- Runs on local Docker Spark cluster or AWS Glue (production)
- Idempotent task design: safe to retry/backfill any execution_date
- Exponential backoff retry strategy

Author: Financial Data Platform Team
"""

import os
import shutil
from datetime import datetime, timedelta
from pathlib import Path

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.task_group import TaskGroup

# Configuration
PROJECT_ROOT = Path(os.getenv('AIRFLOW_HOME', '/opt/airflow'))
SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')
JAR_PATH = PROJECT_ROOT / 'jars' / 'financial-etl-1.0.0.jar'
MAIN_CLASS = 'com.financial.etl.transform.FinancialDataTransform'

# Deployment mode
DEPLOYMENT_MODE = os.getenv('DEPLOYMENT_MODE', 'local')  # 'local' or 'aws'

# Data paths - dynamically switch between local and S3
if DEPLOYMENT_MODE == 'aws':
    RAW_DATA_PATH = os.getenv('S3_RAW_BUCKET', 's3://financial-raw/data/raw')
    CURATED_DATA_PATH = os.getenv('S3_CURATED_BUCKET', 's3://financial-processed/data/curated')
    VALIDATION_PATH = os.getenv('S3_VALIDATION_BUCKET', 's3://financial-validation/reports')
else:
    RAW_DATA_PATH = PROJECT_ROOT / 'data' / 'raw'
    CURATED_DATA_PATH = PROJECT_ROOT / 'data' / 'curated'
    VALIDATION_PATH = PROJECT_ROOT / 'data' / 'validation_reports'

# DAG configuration
DEFAULT_ARGS = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=2),
}

# SLA configuration
SLA = timedelta(hours=3)


def clean_previous_run(**context):
    """
    Idempotency guard: remove any output from a previous run for the same
    execution_date before re-processing. This ensures that retries and
    backfills produce identical results regardless of prior state.

    Cleans:
    - Raw data partition for this execution_date
    - Curated/processed output for this execution_date
    - Daily snapshot for this execution_date
    """
    execution_date = context['ds']

    paths_to_clean = [
        RAW_DATA_PATH / f"date={execution_date}" if not isinstance(RAW_DATA_PATH, str)
        else None,
        CURATED_DATA_PATH / 'daily_snapshots' / f"date={execution_date}" if not isinstance(CURATED_DATA_PATH, str)
        else None,
    ]

    for path in paths_to_clean:
        if path is None:
            continue
        if isinstance(path, Path) and path.exists():
            shutil.rmtree(path)
            print(f"Cleaned previous output: {path}")
        else:
            print(f"No previous output to clean: {path}")

    # For AWS mode, clean S3 paths
    if DEPLOYMENT_MODE == 'aws':
        import boto3
        s3 = boto3.resource('s3')

        for s3_path in [RAW_DATA_PATH, CURATED_DATA_PATH]:
            if isinstance(s3_path, str) and s3_path.startswith('s3://'):
                bucket_name = s3_path.replace('s3://', '').split('/')[0]
                prefix = '/'.join(s3_path.replace('s3://', '').split('/')[1:])
                date_prefix = f"{prefix}/date={execution_date}/"

                bucket = s3.Bucket(bucket_name)
                objects = list(bucket.objects.filter(Prefix=date_prefix))
                if objects:
                    bucket.delete_objects(
                        Delete={'Objects': [{'Key': obj.key} for obj in objects]}
                    )
                    print(f"Cleaned S3 prefix: s3://{bucket_name}/{date_prefix} ({len(objects)} objects)")

    print(f"Idempotency cleanup complete for execution_date={execution_date}")


def check_spark_cluster(**context):
    """
    éªŒè¯ Spark é›†ç¾¤æ˜¯å¦å¯ç”¨
    """
    import socket

    master_host = SPARK_MASTER.split('://')[1].split(':')[0]
    master_port = int(SPARK_MASTER.split(':')[-1])

    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        result = sock.connect_ex((master_host, master_port))
        sock.close()

        if result == 0:
            print(f"âœ… Spark cluster is reachable: {SPARK_MASTER}")
            return True
        else:
            raise ConnectionError(f"Cannot connect to Spark master: {SPARK_MASTER}")

    except Exception as e:
        print(f"âŒ Spark cluster check failed: {e}")
        raise


def verify_jar_exists(**context):
    """
    éªŒè¯ Scala JAR æ–‡ä»¶å­˜åœ¨
    """
    if not JAR_PATH.exists():
        raise FileNotFoundError(
            f"JAR file not found: {JAR_PATH}\n"
            f"Build it with: sbt assembly"
        )

    jar_size = JAR_PATH.stat().st_size / (1024 * 1024)
    print(f"âœ… JAR found: {JAR_PATH} ({jar_size:.2f} MB)")

    return str(JAR_PATH)


def upload_to_s3_if_needed(**context):
    """
    å¦‚æœæ˜¯ AWS éƒ¨ç½²æ¨¡å¼,ä¸Šä¼ ç”Ÿæˆçš„æ•°æ®åˆ° S3
    """
    if DEPLOYMENT_MODE != 'aws':
        print("âœ… Local mode - skip S3 upload")
        return

    import boto3
    from pathlib import Path

    print(f"â˜ï¸  Uploading data to S3...")

    s3_client = boto3.client('s3')
    local_data_dir = PROJECT_ROOT / 'data' / 'raw'

    # Extract bucket name from S3 URL
    bucket_name = RAW_DATA_PATH.replace('s3://', '').split('/')[0]
    s3_prefix = '/'.join(RAW_DATA_PATH.replace('s3://', '').split('/')[1:])

    uploaded_count = 0
    for file_path in local_data_dir.rglob('*.json'):
        s3_key = f"{s3_prefix}/{file_path.relative_to(local_data_dir)}"
        s3_client.upload_file(str(file_path), bucket_name, s3_key)
        uploaded_count += 1

    print(f"âœ… Uploaded {uploaded_count} files to {RAW_DATA_PATH}")


def generate_sample_data(**context):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®ï¼ˆé»˜è®¤æ–¹å¼ï¼‰

    ä¼˜åŠ¿ï¼š
    - æ— éœ€ API Key
    - æ— ç½‘ç»œä¾èµ–
    - æ— é€Ÿç‡é™åˆ¶
    - 100% å¯é 
    - é€‚åˆæ¼”ç¤ºå’Œæµ‹è¯•

    å¦‚éœ€ä½¿ç”¨çœŸå® APIï¼Œè¯·ä½¿ç”¨ fetch_stock_data_from_api() å‡½æ•°
    """
    import subprocess

    execution_date = context['ds']

    print(f"ğŸ² Generating sample data for {execution_date}...")
    print(f"   Deployment mode: {DEPLOYMENT_MODE}")
    print("   (Using simulated data for stable demo)")

    # ç”Ÿæˆåˆ°æœ¬åœ°ä¸´æ—¶ç›®å½•
    local_output = PROJECT_ROOT / 'data' / 'raw' if DEPLOYMENT_MODE == 'aws' else RAW_DATA_PATH

    # è°ƒç”¨æ•°æ®ç”Ÿæˆè„šæœ¬
    cmd = [
        'python',
        'scripts/data_generator.py',
        '--preset', 'demo',  # é»˜è®¤ä½¿ç”¨ demo é¢„è®¾
        '--execution-date', execution_date,
        '--output', str(local_output),
    ]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True,
        )

        print(result.stdout)

        # ç»Ÿè®¡ç”Ÿæˆçš„æ•°æ®
        output_dir = RAW_DATA_PATH / f"date={execution_date}"
        json_files = list(output_dir.glob('**/*.json'))

        total_records = 0
        for json_file in json_files:
            import json
            with open(json_file) as f:
                data = json.load(f)
                total_records += len(data.get('data', []))

        print(f"\nâœ… Generated {total_records} records from {len(json_files)} files")

        # Push to XCom
        context['task_instance'].xcom_push(key='total_records', value=total_records)
        context['task_instance'].xcom_push(key='output_path', value=str(output_dir))
        context['task_instance'].xcom_push(key='data_source', value='simulated')

    except subprocess.CalledProcessError as e:
        print(f"âŒ Error generating data: {e}")
        print(f"STDOUT: {e.stdout}")
        print(f"STDERR: {e.stderr}")
        raise


def fetch_stock_data_from_api(**context):
    """
    ä» Alpha Vantage API è·å–çœŸå®è‚¡ç¥¨æ•°æ®ï¼ˆå¤‡ç”¨æ–¹å¼ï¼‰

    ä½¿ç”¨åœºæ™¯ï¼š
    - éœ€è¦çœŸå®å†å²æ•°æ®
    - ç”Ÿäº§ç¯å¢ƒ
    - å›æµ‹éªŒè¯

    è¦æ±‚ï¼š
    - è®¾ç½® ALPHA_VANTAGE_API_KEY ç¯å¢ƒå˜é‡
    - æ³¨æ„ API é™é¢ï¼ˆ5 req/min, 500 req/dayï¼‰
    """
    import sys
    import json
    from pathlib import Path

    # Add src to path
    sys.path.insert(0, str(PROJECT_ROOT / 'src'))

    from ingestion.alpha_vantage_client import AlphaVantageClient

    # Initialize client
    api_key = os.getenv('ALPHA_VANTAGE_API_KEY')
    if not api_key:
        raise ValueError(
            "ALPHA_VANTAGE_API_KEY not set.\n"
            "Get free API key at: https://www.alphavantage.co/support/#api-key\n"
            "Or use generate_sample_data() for demo purposes."
        )

    client = AlphaVantageClient(api_key=api_key)

    # Symbols to fetch
    symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']

    execution_date = context['ds']
    output_dir = RAW_DATA_PATH / f"date={execution_date}"
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“¥ Fetching REAL data from Alpha Vantage API...")
    print(f"   Symbols: {', '.join(symbols)}")

    total_records = 0
    for symbol in symbols:
        try:
            print(f"  Fetching {symbol}...")
            data = client.get_daily_adjusted(symbol, outputsize='compact')

            if not data:
                print(f"  âš ï¸  No data for {symbol}")
                continue

            # Save to JSON
            symbol_dir = output_dir / f"symbol={symbol}"
            symbol_dir.mkdir(parents=True, exist_ok=True)

            output_file = symbol_dir / f"{symbol}_{execution_date}.json"
            with open(output_file, 'w') as f:
                json.dump(data, f, indent=2)

            records_count = len(data.get('data', []))
            total_records += records_count
            print(f"  âœ… {symbol}: {records_count} records")

        except Exception as e:
            print(f"  âŒ Error fetching {symbol}: {e}")
            continue

    if total_records == 0:
        raise ValueError("No data fetched from API")

    print(f"\nâœ… Total fetched: {total_records} records from Alpha Vantage API")

    # Push to XCom
    context['task_instance'].xcom_push(key='total_records', value=total_records)
    context['task_instance'].xcom_push(key='output_path', value=str(output_dir))
    context['task_instance'].xcom_push(key='data_source', value='alpha_vantage_api')


def verify_ingestion(**context):
    """
    éªŒè¯æ•°æ®æ‘„å–å®Œæˆ
    """
    execution_date = context['ds']
    output_dir = RAW_DATA_PATH / f"date={execution_date}"

    if not output_dir.exists():
        raise FileNotFoundError(f"Output directory not found: {output_dir}")

    json_files = list(output_dir.glob('**/*.json'))

    if not json_files:
        raise ValueError(f"No JSON files found in {output_dir}")

    total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)

    print(f"âœ… Ingestion verified:")
    print(f"   Files: {len(json_files)}")
    print(f"   Total size: {total_size:.2f} MB")
    print(f"   Location: {output_dir}")


def verify_transformation(**context):
    """
    éªŒè¯ Spark è½¬æ¢è¾“å‡º
    """
    execution_date = context['ds']

    # Check processed directory
    processed_dir = CURATED_DATA_PATH / 'processed'
    if not processed_dir.exists():
        raise FileNotFoundError(f"Processed directory not found: {processed_dir}")

    # Check for parquet files
    parquet_files = list(processed_dir.glob('**/*.parquet'))
    if not parquet_files:
        raise ValueError(f"No parquet files found in {processed_dir}")

    total_size = sum(f.stat().st_size for f in parquet_files) / (1024 * 1024)

    print(f"âœ… Transformation verified:")
    print(f"   Parquet files: {len(parquet_files)}")
    print(f"   Total size: {total_size:.2f} MB")

    # Check daily snapshot
    snapshot_path = CURATED_DATA_PATH / 'daily_snapshots' / f"date={execution_date}"
    if snapshot_path.exists():
        snapshot_files = list(snapshot_path.glob('*.parquet'))
        print(f"   Snapshot files: {len(snapshot_files)}")


def load_to_postgres(**context):
    """
    Idempotent load to PostgreSQL (Redshift substitute).

    Uses DELETE-then-INSERT within a single transaction keyed on
    execution_date to guarantee that re-runs for the same date
    produce identical warehouse state (no duplicates).
    """
    import sys
    sys.path.insert(0, str(PROJECT_ROOT / 'src'))

    execution_date = context['ds']
    snapshot_path = CURATED_DATA_PATH / 'daily_snapshots' / f"date={execution_date}"

    # Find parquet file
    parquet_files = list(snapshot_path.glob('*.parquet'))
    if not parquet_files:
        raise FileNotFoundError(f"No parquet files in {snapshot_path}")

    parquet_file = parquet_files[0]

    print(f"Loading from: {parquet_file}")

    import pandas as pd

    df = pd.read_parquet(str(parquet_file))
    rows_to_load = len(df)

    # Idempotent load: DELETE existing rows for this execution_date, then INSERT
    connection_params = {
        'host': 'postgres',
        'database': 'financial_dw',
        'user': 'airflow',
        'password': 'airflow'
    }

    try:
        import psycopg2
        conn = psycopg2.connect(**connection_params)
        conn.autocommit = False
        cursor = conn.cursor()

        # DELETE existing data for this execution_date (idempotency)
        delete_sql = "DELETE FROM fact_stock_prices WHERE trade_date = %s"
        cursor.execute(delete_sql, (execution_date,))
        deleted = cursor.rowcount
        print(f"Deleted {deleted} existing rows for {execution_date} (idempotent cleanup)")

        # INSERT new data
        from loading.postgres_loader import load_parquet_to_postgres
        rows_loaded = load_parquet_to_postgres(
            parquet_path=str(parquet_file),
            table_name='fact_stock_prices',
            connection_params=connection_params
        )

        conn.commit()
        print(f"Loaded {rows_loaded} rows to PostgreSQL (idempotent upsert)")

    except ImportError:
        # Fallback if psycopg2 not available: use simple overwrite loader
        from loading.postgres_loader import load_parquet_to_postgres
        rows_loaded = load_parquet_to_postgres(
            parquet_path=str(parquet_file),
            table_name='fact_stock_prices',
            connection_params=connection_params
        )
        print(f"Loaded {rows_loaded} rows to PostgreSQL (fallback mode)")

    except Exception as e:
        if 'conn' in locals():
            conn.rollback()
        raise
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

    context['task_instance'].xcom_push(key='rows_loaded', value=rows_loaded)


def send_success_notification(**context):
    """
    å‘é€æˆåŠŸé€šçŸ¥
    """
    execution_date = context['ds']
    total_records = context['task_instance'].xcom_pull(
        task_ids='fetch_stock_data',
        key='total_records'
    )
    rows_loaded = context['task_instance'].xcom_pull(
        task_ids='load_to_postgres',
        key='rows_loaded'
    )

    message = f"""
âœ… Financial Data Pipeline Success

Execution Date: {execution_date}
Records Fetched: {total_records}
Records Loaded: {rows_loaded}

Scala + Spark transformation completed successfully!
    """

    print(message)

    # Optional: Send to Slack
    # slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
    # if slack_webhook:
    #     import requests
    #     requests.post(slack_webhook, json={'text': message})


# ============================================================================
# DAG Definition
# ============================================================================

with DAG(
    dag_id='financial_data_pipeline_scala',
    default_args=DEFAULT_ARGS,
    description='Financial data ETL pipeline with Scala + Spark',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['scala', 'spark', 'etl', 'financial'],
    max_active_runs=1,
    sla_miss_callback=None,  # Add custom SLA callback if needed
) as dag:

    # Start marker
    start = DummyOperator(task_id='start')

    # Idempotency guard: clean any previous output for this execution_date
    clean = PythonOperator(
        task_id='clean_previous_run',
        python_callable=clean_previous_run,
        doc_md="""
        ### Idempotency Guard
        Removes any output from a previous run for the same execution_date.
        Ensures retries and backfills produce identical results.
        """
    )

    # Pre-flight checks
    with TaskGroup('pre_flight_checks', tooltip='Verify environment configuration') as pre_flight:

        check_spark = PythonOperator(
            task_id='check_spark_cluster',
            python_callable=check_spark_cluster,
            doc_md="""
            ### Check Spark Cluster
            éªŒè¯ Spark é›†ç¾¤æ˜¯å¦å¯ç”¨
            """
        )

        check_jar = PythonOperator(
            task_id='verify_jar_exists',
            python_callable=verify_jar_exists,
            doc_md="""
            ### Verify JAR
            éªŒè¯ Scala JAR æ–‡ä»¶å­˜åœ¨
            """
        )

        check_spark >> check_jar

    # Data ingestion - é»˜è®¤ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    fetch = PythonOperator(
        task_id='generate_sample_data',
        python_callable=generate_sample_data,  # æ”¹ä¸ºä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
        sla=timedelta(minutes=10),
        doc_md="""
        ### Generate Sample Data
        ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®ï¼ˆæ¨èç”¨äºæ¼”ç¤ºï¼‰

        - Symbols: AAPL, GOOGL, MSFT, AMZN, TSLA
        - Data: æ¨¡æ‹ŸçœŸå®å¸‚åœºè¡Œä¸ºï¼ˆä»·æ ¼æ³¢åŠ¨ã€åˆ†çº¢ã€æ‹†è‚¡ï¼‰
        - Output: JSON files in data/raw/
        - ä¼˜åŠ¿: æ— éœ€ API Key, 100% ç¨³å®š, æ— é€Ÿç‡é™åˆ¶

        å¦‚éœ€ä½¿ç”¨çœŸå® API æ•°æ®ï¼Œè¯·æ›¿æ¢ä¸º fetch_stock_data_from_api
        """
    )

    # Upload to S3 if AWS mode
    upload_s3 = PythonOperator(
        task_id='upload_to_s3',
        python_callable=upload_to_s3_if_needed,
        doc_md="""
        ### Upload to S3 (AWS mode only)
        å¦‚æœ DEPLOYMENT_MODE=awsï¼Œä¸Šä¼ æ•°æ®åˆ° S3
        """
    )

    verify_fetch = PythonOperator(
        task_id='verify_ingestion',
        python_callable=verify_ingestion,
        doc_md="""
        ### Verify Ingestion
        éªŒè¯æ•°æ®æ‘„å–å®Œæˆ
        """
    )

    # Scala Spark transformation
    spark_transform = SparkSubmitOperator(
        task_id='scala_spark_transform',
        application=str(JAR_PATH),
        java_class=MAIN_CLASS,
        conn_id='spark_default',
        conf={
            'spark.sql.shuffle.partitions': '8',
            'spark.sql.adaptive.enabled': 'true',
            'spark.sql.adaptive.coalescePartitions.enabled': 'true',
        },
        driver_memory='1g',
        executor_memory='2g',
        executor_cores=2,
        num_executors=2,
        application_args=[
            '--source-path', str(RAW_DATA_PATH),
            '--target-path', str(CURATED_DATA_PATH),
            '--execution-date', '{{ ds }}',
        ],
        sla=timedelta(hours=1),
        doc_md="""
        ### Scala Spark Transformation

        è¿è¡Œ Scala + Spark ETL ä½œä¸š:
        - è¯»å– JSON åŸå§‹æ•°æ®
        - æ•°æ®æ¸…æ´—å’ŒéªŒè¯
        - è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (SMA, EMA, æ³¢åŠ¨ç‡)
        - å†™å…¥ Parquet æ–‡ä»¶

        **Language:** Scala 2.12
        **Framework:** Apache Spark 3.4.1
        """
    )

    verify_transform = PythonOperator(
        task_id='verify_transformation',
        python_callable=verify_transformation,
        doc_md="""
        ### Verify Transformation
        éªŒè¯ Spark è½¬æ¢è¾“å‡º
        """
    )

    # Load to data warehouse
    load = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres,
        sla=timedelta(minutes=30),
        doc_md="""
        ### Load to PostgreSQL
        å°† Parquet æ•°æ®åŠ è½½åˆ° PostgreSQL
        """
    )

    # Success notification
    notify = PythonOperator(
        task_id='send_success_notification',
        python_callable=send_success_notification,
        trigger_rule='all_success',
        doc_md="""
        ### Success Notification
        å‘é€æˆåŠŸé€šçŸ¥
        """
    )

    # End marker
    end = DummyOperator(task_id='end')

    # Define task dependencies
    # clean_previous_run ensures idempotency before any data is produced
    start >> clean >> pre_flight >> fetch >> upload_s3 >> verify_fetch
    verify_fetch >> spark_transform >> verify_transform
    verify_transform >> load >> notify >> end


# ============================================================================
# DAG Documentation
# ============================================================================

dag.doc_md = """
# Financial Data Pipeline - Scala + Spark Version

## æ¦‚è¿°

ä½¿ç”¨ **Scala + Spark** çš„ç”Ÿäº§çº§é‡‘èæ•°æ® ETL ç®¡é“ã€‚

## æ¶æ„

```
Alpha Vantage API
      â†“
  [Python] fetch_stock_data
      â†“
  JSON â†’ data/raw/
      â†“
  [Scala + Spark] spark_transform
      â†“
  Parquet â†’ data/curated/
      â†“
  [Python] load_to_postgres
      â†“
  PostgreSQL
```

## æŠ€æœ¯æ ˆ

- **ç¼–æ’**: Apache Airflow
- **æ•°æ®å¤„ç†**: Scala + Apache Spark 3.4.1
- **å­˜å‚¨æ ¼å¼**: Parquet (Snappy å‹ç¼©)
- **æ•°æ®ä»“åº“**: PostgreSQL (æœ¬åœ°) / Redshift (ç”Ÿäº§)

## æ ¸å¿ƒåŠŸèƒ½

### Scala Spark è½¬æ¢
- âœ… ç±»å‹å®‰å…¨çš„æ•°æ®å¤„ç†
- âœ… é«˜æ€§èƒ½å¹¶è¡Œè®¡ç®—
- âœ… å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
- âœ… ä¼˜åŒ–çš„ Parquet å†™å…¥

### æŠ€æœ¯æŒ‡æ ‡
- Simple Moving Averages (SMA): 5, 20, 50 å¤©
- Exponential Moving Averages (EMA): 12, 26 å¤©
- æ³¢åŠ¨ç‡ (Volatility): 20 å¤©æ ‡å‡†å·®
- æ—¥æ”¶ç›Šç‡ã€ä»·æ ¼åŒºé—´ã€æˆäº¤é‡å˜åŒ–

## è¿è¡Œç¯å¢ƒ

### æœ¬åœ° Docker
```bash
# å¯åŠ¨ Spark é›†ç¾¤
docker-compose -f docker-compose-spark.yml up -d

# æ„å»º Scala JAR
sbt assembly

# è§¦å‘ DAG
airflow dags trigger financial_data_pipeline_scala
```

### AWS ç”Ÿäº§ç¯å¢ƒ
- æ›¿æ¢ PostgreSQL â†’ Redshift
- æ›¿æ¢æœ¬åœ° Spark â†’ AWS Glue
- æ·»åŠ  S3 ä½œä¸ºæ•°æ®å­˜å‚¨

## ç›‘æ§

- Airflow UI: http://localhost:8080
- Spark Master UI: http://localhost:8081
- Spark Application UI: http://localhost:4040

## æˆæœ¬ä¼˜åŒ–

**æœ¬åœ°å¼€å‘**: $0/æœˆ
- Docker Compose è¿è¡Œå…¨éƒ¨æœåŠ¡
- å®Œå…¨å…è´¹

**AWS ç”Ÿäº§**: å¯é€šè¿‡ä»¥ä¸‹ä¼˜åŒ–é™ä½æˆæœ¬
- ä½¿ç”¨ Glue Flex æ‰§è¡Œç±»
- Redshift è‡ªåŠ¨æš‚åœ
- S3 ç”Ÿå‘½å‘¨æœŸç­–ç•¥

## ç»´æŠ¤

- å®šæœŸæ£€æŸ¥ Spark æ—¥å¿—
- ç›‘æ§ Parquet æ–‡ä»¶å¤§å°
- éªŒè¯æ•°æ®è´¨é‡æŒ‡æ ‡
"""
