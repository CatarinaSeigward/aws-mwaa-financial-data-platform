# Scala + Spark å®æ–½æŒ‡å—

## ğŸ¯ æ”¹é€ æ€»ç»“

é¡¹ç›®å·²ä» **PySpark** æ”¹ä¸º **çº¯ Scala + Spark**ï¼Œå…·å¤‡ä»¥ä¸‹ä¼˜åŠ¿ï¼š

### âœ… æŠ€æœ¯ä¼˜åŠ¿

| ç‰¹æ€§ | PySpark | Scala + Spark |
|------|---------|--------------|
| **ç±»å‹å®‰å…¨** | âŒ è¿è¡Œæ—¶æ£€æŸ¥ | âœ… **ç¼–è¯‘æ—¶æ£€æŸ¥** |
| **æ€§èƒ½** | ä¸­ç­‰ (Python â†’ JVM) | âœ… **é«˜æ€§èƒ½ (åŸç”Ÿ JVM)** |
| **ä»£ç è´¨é‡** | åŠ¨æ€ç±»å‹ | âœ… **é™æ€ç±»å‹ + æ¨¡å¼åŒ¹é…** |
| **éƒ¨ç½²** | ä¾èµ– Python | âœ… **å•ä¸ª JAR æ–‡ä»¶** |
| **é¢è¯•å±•ç¤º** | å¸¸è§ | âœ… **æ›´æ˜¾ä¸“ä¸šåº¦** |
| **ä¼ä¸šé‡‡ç”¨** | æ•°æ®ç§‘å­¦ | âœ… **å¤§æ•°æ®å·¥ç¨‹** |

### ğŸ’° æˆæœ¬å¯¹æ¯”

| ç¯å¢ƒ | æˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **æœ¬åœ° Docker** | **$0/æœˆ** | âœ… æ¨èæ±‚èŒæ¼”ç¤º |
| **AWS Glue (ä¼˜åŒ–)** | $500-800/æœˆ | ç”Ÿäº§ç¯å¢ƒ |
| **AWS åŸé…ç½®** | $36K-135K/æœˆ | âŒ å¤ªè´µ |

---

## ğŸ“¦ é¡¹ç›®ç»“æ„

```
financial-data-platform/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ transformation/
â”‚       â””â”€â”€ scala/
â”‚           â””â”€â”€ FinancialDataTransform.scala  â† ä¸» ETL é€»è¾‘
â”‚
â”œâ”€â”€ build.sbt                                  â† Scala é¡¹ç›®é…ç½®
â”œâ”€â”€ project/
â”‚   â”œâ”€â”€ build.properties                       â† SBT ç‰ˆæœ¬
â”‚   â””â”€â”€ plugins.sbt                            â† SBT æ’ä»¶
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ financial_data_pipeline_scala.py      â† Airflow DAG
â”‚
â”œâ”€â”€ docker-compose-spark.yml                   â† Spark é›†ç¾¤é…ç½®
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ build-and-submit.sh                    â† æ„å»º+æäº¤è„šæœ¬
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ raw/                                   â† JSON åŸå§‹æ•°æ®
    â””â”€â”€ curated/                               â† Parquet è¾“å‡º
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

#### å¿…éœ€ (æœ¬åœ°è¿è¡Œ)
```bash
# 1. Docker + Docker Compose
docker --version  # 20.10+
docker-compose --version  # 1.29+

# 2. Java JDK
java -version  # Java 11 æˆ– 17

# 3. Scala Build Tool (SBT)
sbt --version  # 1.9+
```

#### å®‰è£… SBT

**macOS:**
```bash
brew install sbt
```

**Ubuntu/Debian:**
```bash
echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list
curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add
sudo apt-get update
sudo apt-get install sbt
```

**Windows:**
```bash
choco install sbt
# æˆ–ä¸‹è½½å®‰è£…: https://www.scala-sbt.org/download.html
```

---

## ğŸ—ï¸ æ­¥éª¤ 1: æ„å»º Scala JAR

### æ–¹æ³• A: ä½¿ç”¨è„šæœ¬ (æ¨è)

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd aws-mwaa-financial-data-platform

# æ„å»º JAR
./scripts/build-and-submit.sh local --rebuild
```

### æ–¹æ³• B: æ‰‹åŠ¨æ„å»º

```bash
# æ¸…ç†
sbt clean

# ç¼–è¯‘
sbt compile

# è¿è¡Œæµ‹è¯•
sbt test

# åˆ›å»º fat JAR (åŒ…å«æ‰€æœ‰ä¾èµ–)
sbt assembly

# è¾“å‡º: target/scala-2.12/financial-etl-1.0.0.jar
```

**é¢„æœŸè¾“å‡º:**
```
[info] Strategy 'discard' was applied to 2 files
[info] Strategy 'concat' was applied to 2 files
[success] Total time: 45 s
[success] Built JAR: target/scala-2.12/financial-etl-1.0.0.jar (35.2 MB)
```

---

## ğŸ³ æ­¥éª¤ 2: å¯åŠ¨ Docker ç¯å¢ƒ

### å¯åŠ¨æ‰€æœ‰æœåŠ¡

```bash
# å¯åŠ¨ Spark é›†ç¾¤ + Airflow + PostgreSQL
docker-compose -f docker-compose-spark.yml up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose -f docker-compose-spark.yml ps
```

**é¢„æœŸæœåŠ¡:**
```
NAME                      STATUS    PORTS
financial-spark-master    Up        0.0.0.0:7077->7077/tcp, 0.0.0.0:8081->8081/tcp
financial-spark-worker-1  Up
financial-spark-worker-2  Up
financial-airflow         Up        0.0.0.0:8080->8080/tcp
financial-postgres        Up        0.0.0.0:5432->5432/tcp
financial-grafana         Up        0.0.0.0:3000->3000/tcp
financial-minio           Up        0.0.0.0:9000-9001->9000-9001/tcp
```

### è®¿é—® Web UI

| æœåŠ¡ | URL | ç”¨æˆ·å | å¯†ç  |
|------|-----|--------|------|
| **Airflow** | http://localhost:8080 | admin | admin |
| **Spark Master** | http://localhost:8081 | - | - |
| **Grafana** | http://localhost:3000 | admin | admin |
| **MinIO** | http://localhost:9001 | minioadmin | minioadmin |

---

## ğŸ¬ æ­¥éª¤ 3: è¿è¡Œ ETL æµç¨‹

### æ–¹æ³• A: é€šè¿‡ Airflow DAG (æ¨è)

1. **æ‰“å¼€ Airflow UI**
   ```
   http://localhost:8080
   ç™»å½•: admin / admin
   ```

2. **æ‰¾åˆ° DAG**
   - DAG ID: `financial_data_pipeline_scala`
   - æ ‡ç­¾: `scala`, `spark`, `etl`, `financial`

3. **é…ç½® Alpha Vantage API Key**
   ```bash
   # ç¼–è¾‘ .env æ–‡ä»¶
   echo "ALPHA_VANTAGE_API_KEY=your_key_here" >> .env

   # é‡å¯ Airflow
   docker-compose -f docker-compose-spark.yml restart airflow
   ```

   è·å–å…è´¹ API Key: https://www.alphavantage.co/support/#api-key

4. **æ‰‹åŠ¨è§¦å‘ DAG**
   - ç‚¹å‡» DAG åç§°
   - ç‚¹å‡»å³ä¸Šè§’ "Trigger DAG" â–¶ï¸ æŒ‰é’®
   - ç­‰å¾…æ‰§è¡Œå®Œæˆ (~5-10åˆ†é’Ÿ)

5. **æŸ¥çœ‹æ‰§è¡Œç»“æœ**
   - ä»»åŠ¡çŠ¶æ€: ç»¿è‰² = æˆåŠŸ
   - ç‚¹å‡»ä»»åŠ¡æŸ¥çœ‹æ—¥å¿—
   - æ£€æŸ¥ Spark UI: http://localhost:8081

### æ–¹æ³• B: ç›´æ¥è¿è¡Œ Spark Job

```bash
# æœ¬åœ°æ¨¡å¼
./scripts/build-and-submit.sh local

# é›†ç¾¤æ¨¡å¼
./scripts/build-and-submit.sh cluster
```

### æ–¹æ³• C: æ‰‹åŠ¨ spark-submit

```bash
# æœ¬åœ°æ¨¡å¼
spark-submit \
  --class com.financial.etl.transform.FinancialDataTransform \
  --master "local[*]" \
  --driver-memory 2g \
  target/scala-2.12/financial-etl-1.0.0.jar \
  --source-path ./data/raw \
  --target-path ./data/curated \
  --execution-date 2024-01-15

# Docker é›†ç¾¤æ¨¡å¼
docker exec financial-spark-master \
  spark-submit \
  --class com.financial.etl.transform.FinancialDataTransform \
  --master "spark://spark-master:7077" \
  --driver-memory 1g \
  --executor-memory 2g \
  --total-executor-cores 4 \
  /jars/financial-etl-1.0.0.jar \
  --source-path /data/raw \
  --target-path /data/curated \
  --execution-date 2024-01-15
```

---

## ğŸ“Š æ­¥éª¤ 4: éªŒè¯è¾“å‡º

### æ£€æŸ¥ Parquet æ–‡ä»¶

```bash
# æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶
find data/curated/processed -name "*.parquet"

# è¾“å‡ºç¤ºä¾‹:
# data/curated/processed/year=2024/month=1/part-00000.snappy.parquet
# data/curated/processed/year=2024/month=1/part-00001.snappy.parquet
```

### æŸ¥è¯¢ PostgreSQL

```bash
# è¿æ¥æ•°æ®åº“
docker exec -it financial-postgres psql -U airflow -d financial_dw

# æŸ¥è¯¢æ•°æ®
SELECT
  symbol,
  COUNT(*) as record_count,
  MIN(trade_date) as earliest_date,
  MAX(trade_date) as latest_date,
  AVG(close_price) as avg_close
FROM fact_stock_prices
GROUP BY symbol;

# æŸ¥çœ‹æŠ€æœ¯æŒ‡æ ‡
SELECT
  symbol,
  trade_date,
  close_price,
  sma_5,
  sma_20,
  daily_return,
  volatility_20d
FROM fact_stock_prices
ORDER BY trade_date DESC
LIMIT 10;
```

### ä½¿ç”¨ Pandas è¯»å– Parquet

```python
import pandas as pd

# è¯»å– Parquet
df = pd.read_parquet('data/curated/processed/')

print(f"Total records: {len(df)}")
print(f"Columns: {df.columns.tolist()}")
print(f"\nSample data:")
print(df.head())

# æŠ€æœ¯æŒ‡æ ‡ç»Ÿè®¡
print(f"\nTechnical Indicators:")
print(df[['symbol', 'sma_5', 'sma_20', 'ema_12', 'volatility_20d']].describe())
```

---

## ğŸ” ä»£ç è¯¦è§£

### Scala æ ¸å¿ƒè½¬æ¢é€»è¾‘

**æ–‡ä»¶**: `src/transformation/scala/FinancialDataTransform.scala`

#### ä¸»è¦åŠŸèƒ½æ¨¡å—

```scala
// 1. æ•°æ®è¯»å–
def readRawData(spark: SparkSession, path: String): DataFrame = {
  spark.read
    .option("multiLine", "true")
    .json(path)
    .withColumn("source_file", input_file_name())
}

// 2. æ•°æ®æ¸…æ´—
def cleanseData(df: DataFrame): DataFrame = {
  df.filter(col("symbol").isNotNull && col("timestamp").isNotNull)
    .withColumn("symbol", upper(trim(col("symbol"))))
    .withColumn("volume", coalesce(col("volume"), lit(0L)))
    .dropDuplicates("symbol", "timestamp")
}

// 3. æŠ€æœ¯æŒ‡æ ‡è®¡ç®— (SMA)
def calculateMovingAverages(df: DataFrame, periods: Seq[Int]): DataFrame = {
  var result = df
  periods.foreach { period =>
    val windowSpec = Window
      .partitionBy("symbol")
      .orderBy("trade_date")
      .rowsBetween(-period + 1, 0)

    result = result.withColumn(
      s"sma_$period",
      round(avg("close_price").over(windowSpec), 4)
    )
  }
  result
}

// 4. Parquet å†™å…¥
def writeToCurated(df: DataFrame, path: String, partitionCols: Seq[String]): Unit = {
  df.write
    .mode(SaveMode.Overwrite)
    .partitionBy(partitionCols: _*)
    .parquet(path)
}
```

#### ç±»å‹å®‰å…¨çš„å¥½å¤„

```scala
// âŒ PySpark - è¿è¡Œæ—¶æ‰æŠ¥é”™
df.withColumn("typo_column", col("clse_price") * 2)  // ç¼–è¯‘é€šè¿‡ï¼Œè¿è¡ŒæŠ¥é”™

// âœ… Scala - ç¼–è¯‘æ—¶æ£€æŸ¥
df.withColumn("new_column", col("close_price") * 2)  // ç±»å‹æ£€æŸ¥
```

---

## ğŸ“ æŠ€èƒ½å±•ç¤ºè¦ç‚¹

### å¯¹æ¯” PySpark

åœ¨é¢è¯•ä¸­ï¼Œä½ å¯ä»¥è¿™æ ·è¯´æ˜ï¼š

> "æœ€åˆä½¿ç”¨ PySpark å®ç°åŸå‹ï¼Œä½†ä¸ºäº†æå‡æ€§èƒ½å’Œç±»å‹å®‰å…¨ï¼Œé‡æ„ä¸ºçº¯ Scala + Sparkã€‚ä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼š"
>
> 1. **ç±»å‹å®‰å…¨**: ç¼–è¯‘æ—¶æ•è·é”™è¯¯ï¼Œå‡å°‘è¿è¡Œæ—¶å¼‚å¸¸
> 2. **æ€§èƒ½**: åŸç”Ÿ JVM æ‰§è¡Œï¼Œæ—  Python â†” JVM åºåˆ—åŒ–å¼€é”€
> 3. **ä»£ç è´¨é‡**: å‡½æ•°å¼ç¼–ç¨‹ã€æ¨¡å¼åŒ¹é…ã€ä¸å¯å˜æ•°æ®ç»“æ„
> 4. **éƒ¨ç½²ç®€åŒ–**: å•ä¸ª fat JARï¼Œæ—  Python ç¯å¢ƒä¾èµ–
> 5. **ä¼ä¸šæ ‡å‡†**: å¤§æ•°æ®å·¥ç¨‹å›¢é˜Ÿæ™®éä½¿ç”¨ Scala

### æ ¸å¿ƒå®ç°äº®ç‚¹

**1. çª—å£å‡½æ•°ä¼˜åŒ–**
```scala
// ä½¿ç”¨ Window å‡½æ•°è®¡ç®—ç§»åŠ¨å¹³å‡
val windowSpec = Window
  .partitionBy("symbol")
  .orderBy("trade_date")
  .rowsBetween(-19, 0)  // æ»šåŠ¨ 20 å¤©çª—å£

df.withColumn("sma_20", avg("close_price").over(windowSpec))
```

**2. åˆ†åŒºå†™å…¥ç­–ç•¥**
```scala
// æŒ‰å¹´æœˆåˆ†åŒºï¼Œä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
df.write
  .partitionBy("year", "month")
  .parquet(outputPath)

// ç›®å½•ç»“æ„:
// curated/processed/year=2024/month=1/*.parquet
// curated/processed/year=2024/month=2/*.parquet
```

**3. Adaptive Query Execution**
```scala
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

// è‡ªåŠ¨ä¼˜åŒ–:
// - åŠ¨æ€è°ƒæ•´ shuffle åˆ†åŒºæ•°
// - åˆå¹¶å°æ–‡ä»¶
// - å€¾æ–œè¿æ¥å¤„ç†
```

---

## ğŸ› å¸¸è§é—®é¢˜

### 1. SBT æ„å»ºå¤±è´¥

**é—®é¢˜**: `java.lang.OutOfMemoryError: Java heap space`

**è§£å†³**:
```bash
# å¢åŠ  SBT å†…å­˜
export SBT_OPTS="-Xmx4G -XX:+UseG1GC"
sbt assembly
```

### 2. Spark è¿æ¥å¤±è´¥

**é—®é¢˜**: `Cannot connect to spark://spark-master:7077`

**æ£€æŸ¥**:
```bash
# éªŒè¯ Spark Master æ˜¯å¦è¿è¡Œ
docker ps | grep spark-master

# æŸ¥çœ‹æ—¥å¿—
docker logs financial-spark-master

# é‡å¯æœåŠ¡
docker-compose -f docker-compose-spark.yml restart
```

### 3. JAR æ–‡ä»¶æ‰¾ä¸åˆ°

**é—®é¢˜**: `FileNotFoundError: JAR file not found`

**è§£å†³**:
```bash
# é‡æ–°æ„å»º JAR
sbt clean assembly

# å¤åˆ¶åˆ° Docker volume
docker cp target/scala-2.12/financial-etl-1.0.0.jar \
  financial-spark-master:/jars/
```

### 4. Alpha Vantage API é™æµ

**é—®é¢˜**: `API rate limit exceeded (5 calls/min)`

**è§£å†³**:
```python
# åœ¨ fetch_stock_data ä¸­æ·»åŠ å»¶è¿Ÿ
import time
for symbol in symbols:
    data = client.get_daily_adjusted(symbol)
    time.sleep(12)  # ç­‰å¾… 12 ç§’ = 5 æ¬¡/åˆ†é’Ÿ
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### æœ¬åœ°å¼€å‘ä¼˜åŒ–

```scala
// build.sbt ä¼˜åŒ–
Test / fork := true
Test / javaOptions ++= Seq(
  "-Xmx2G",
  "-XX:+UseG1GC",
  "-XX:MaxMetaspaceSize=512m"
)
```

### Spark é…ç½®ä¼˜åŒ–

```bash
spark-submit \
  --conf spark.sql.shuffle.partitions=8 \        # å‡å°‘ shuffle åˆ†åŒº
  --conf spark.default.parallelism=4 \           # å¹¶è¡Œåº¦
  --conf spark.sql.files.maxPartitionBytes=128m \  # æ–‡ä»¶åˆ†åŒºå¤§å°
  --conf spark.sql.adaptive.enabled=true \       # è‡ªé€‚åº”æ‰§è¡Œ
  ...
```

### Parquet å‹ç¼©ä¼˜åŒ–

```scala
// åœ¨ FinancialDataTransform.scala ä¸­å·²é…ç½®
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

// å‹ç¼©ç‡å¯¹æ¯”:
// JSON: 100 MB
// Parquet (Snappy): 30 MB (èŠ‚çœ 70%)
// Parquet (Gzip): 20 MB (æ›´æ…¢ï¼Œæ›´å°)
```

---

## ğŸš€ éƒ¨ç½²åˆ° AWS

### ä¸Šä¼  JAR åˆ° S3

```bash
# æ„å»ºç”Ÿäº§ JAR
sbt clean assembly

# ä¸Šä¼ åˆ° S3
aws s3 cp target/scala-2.12/financial-etl-1.0.0.jar \
  s3://your-bucket/glue-jars/
```

### åˆ›å»º AWS Glue Job

```bash
aws glue create-job \
  --name financial-data-transform \
  --role arn:aws:iam::123456789012:role/GlueServiceRole \
  --command '{
    "Name": "glueetl",
    "ScriptLocation": "s3://your-bucket/glue-jars/financial-etl-1.0.0.jar",
    "PythonVersion": "3"
  }' \
  --default-arguments '{
    "--job-language": "scala",
    "--class": "com.financial.etl.transform.FinancialDataTransform",
    "--source-path": "s3://your-bucket/raw/",
    "--target-path": "s3://your-bucket/curated/"
  }' \
  --glue-version "4.0" \
  --worker-type "G.1X" \
  --number-of-workers 2
```

### ä» Airflow è§¦å‘ Glue

```python
from airflow.providers.amazon.aws.operators.glue import GlueJobOperator

glue_task = GlueJobOperator(
    task_id='run_scala_spark_glue',
    job_name='financial-data-transform',
    script_args={
        '--execution-date': '{{ ds }}'
    },
    aws_conn_id='aws_default'
)
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### Scala + Spark

- **å®˜æ–¹æ–‡æ¡£**: https://spark.apache.org/docs/latest/api/scala/
- **Scala å­¦ä¹ **: https://docs.scala-lang.org/tour/tour-of-scala.html
- **Spark By Examples**: https://sparkbyexamples.com/

### æ¨èä¹¦ç±

- *Learning Spark* (O'Reilly) - Spark åŸºç¡€
- *High Performance Spark* - æ€§èƒ½ä¼˜åŒ–
- *Scala for the Impatient* - Scala å¿«é€Ÿå…¥é—¨

---

## âœ… æ£€æŸ¥æ¸…å•

### æœ¬åœ°å¼€å‘ç¯å¢ƒ

- [ ] Java 11+ å·²å®‰è£…
- [ ] SBT 1.9+ å·²å®‰è£…
- [ ] Docker + Docker Compose å·²å®‰è£…
- [ ] Alpha Vantage API Key å·²è·å–
- [ ] æˆåŠŸæ„å»º JAR (`sbt assembly`)
- [ ] Docker æœåŠ¡å·²å¯åŠ¨
- [ ] å¯ä»¥è®¿é—® Airflow UI (localhost:8080)
- [ ] å¯ä»¥è®¿é—® Spark UI (localhost:8081)

### åŠŸèƒ½éªŒè¯

- [ ] Scala ä»£ç ç¼–è¯‘é€šè¿‡
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡ (`sbt test`)
- [ ] æœ¬åœ° Spark è¿è¡ŒæˆåŠŸ
- [ ] Docker Spark é›†ç¾¤è¿è¡ŒæˆåŠŸ
- [ ] Airflow DAG è§¦å‘æˆåŠŸ
- [ ] Parquet æ–‡ä»¶å·²ç”Ÿæˆ
- [ ] PostgreSQL æ•°æ®å·²åŠ è½½
- [ ] æŠ€æœ¯æŒ‡æ ‡è®¡ç®—æ­£ç¡®

### æ±‚èŒå‡†å¤‡

- [ ] å¯ä»¥æµç•…æ¼”ç¤ºæ•´ä¸ªæµç¨‹ (< 5 åˆ†é’Ÿ)
- [ ] å¯ä»¥è§£é‡Š Scala vs PySpark ä¼˜åŠ¿
- [ ] å¯ä»¥è§£é‡Šçª—å£å‡½æ•°å®ç°
- [ ] å¯ä»¥è§£é‡Šæˆæœ¬ä¼˜åŒ–ç­–ç•¥
- [ ] å‡†å¤‡å¥½æ¶æ„å›¾å’Œä»£ç è®²è§£
- [ ] GitHub README å·²æ›´æ–°

---

## ğŸ¯ æ€»ç»“

é€šè¿‡è¿™æ¬¡æ”¹é€ ï¼Œé¡¹ç›®ç°åœ¨å±•ç¤ºäº†ï¼š

### æŠ€æœ¯æ·±åº¦
âœ… **Scala** - å‡½æ•°å¼ç¼–ç¨‹ã€ç±»å‹å®‰å…¨
âœ… **Spark** - åˆ†å¸ƒå¼è®¡ç®—ã€çª—å£å‡½æ•°
âœ… **Parquet** - åˆ—å¼å­˜å‚¨ä¼˜åŒ–
âœ… **Docker** - å®¹å™¨åŒ–éƒ¨ç½²
âœ… **Airflow** - å·¥ä½œæµç¼–æ’

### æˆæœ¬æ„è¯†
âœ… ä» **$36K-135K/æœˆ** é™è‡³ **$0/æœˆ** (æœ¬åœ°)
âœ… AWS ç”Ÿäº§ç¯å¢ƒå¯ä¼˜åŒ–è‡³ **$500-800/æœˆ**

### ä¸“ä¸šåº¦
âœ… ä¼ä¸šçº§ä»£ç è´¨é‡
âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†
âœ… ç”Ÿäº§å°±ç»ªçš„æ¶æ„
âœ… è¯¦ç»†çš„æ–‡æ¡£

**å®Œç¾çš„æ±‚èŒä½œå“é›†ï¼** ğŸš€

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

é‡åˆ°é—®é¢˜è¯·æ£€æŸ¥:
1. Docker æ—¥å¿—: `docker logs financial-spark-master`
2. Airflow æ—¥å¿—: Airflow UI â†’ DAG â†’ Task â†’ Logs
3. Spark UI: http://localhost:8081

ç¥ä½ é¢è¯•æˆåŠŸï¼ğŸ‰
