"""
Local Pandas ETL Transform (æ›¿ä»£ AWS Glue PySpark)
=================================================
å®Œå…¨åŸºäº Pandas/DuckDB çš„æ•°æ®è½¬æ¢ï¼Œæ— éœ€ Spark/Scala

ç‰¹ç‚¹:
- âœ… é›¶ Spark ä¾èµ–
- âœ… é›¶ JVM ä¾èµ–
- âœ… é›¶æˆæœ¬è¿è¡Œ
- âœ… ä¿ç•™æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
- âœ… å¯åœ¨ç¬”è®°æœ¬ä¸Šè¿è¡Œ

Usage:
    python local_pandas_transform.py \
        --source_path ./data/raw/2024-01-15 \
        --target_path ./data/curated/2024-01-15.parquet
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq

# Optional: DuckDB for SQL-like operations (å®Œå…¨å…è´¹)
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError:
    DUCKDB_AVAILABLE = False
    print("DuckDB not installed. Using Pandas only.")


# ==============================================================================
# Configuration
# ==============================================================================
class Config:
    """è½¬æ¢é…ç½®"""
    SMA_PERIODS = [5, 20, 50]
    EMA_PERIODS = [12, 26]
    VOLATILITY_WINDOW = 20
    PARQUET_COMPRESSION = 'snappy'
    CHUNK_SIZE = 10000  # åˆ†å—å¤„ç†å¤§æ–‡ä»¶


# ==============================================================================
# Data Reading Functions
# ==============================================================================
def read_json_files(source_path: str) -> pd.DataFrame:
    """
    ä»ç›®å½•è¯»å–æ‰€æœ‰ JSON æ–‡ä»¶

    Args:
        source_path: åŒ…å« JSON æ–‡ä»¶çš„ç›®å½•è·¯å¾„

    Returns:
        åˆå¹¶åçš„ DataFrame
    """
    print(f"ğŸ“– Reading JSON files from: {source_path}")

    source_path = Path(source_path)
    json_files = list(source_path.glob('**/*.json'))

    if not json_files:
        raise FileNotFoundError(f"No JSON files found in {source_path}")

    print(f"Found {len(json_files)} JSON files")

    dfs = []
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # å¤„ç†åµŒå¥—ç»“æ„
            if isinstance(data, dict) and 'data' in data:
                df = pd.DataFrame(data['data'])
            elif isinstance(data, list):
                df = pd.DataFrame(data)
            else:
                df = pd.DataFrame([data])

            df['source_file'] = str(json_file)
            dfs.append(df)

        except Exception as e:
            print(f"âš ï¸  Error reading {json_file}: {e}")
            continue

    if not dfs:
        raise ValueError("No valid data found in JSON files")

    combined_df = pd.concat(dfs, ignore_index=True)
    print(f"âœ… Loaded {len(combined_df)} records from {len(json_files)} files")

    return combined_df


def read_from_s3_to_local(s3_path: str, local_path: str):
    """
    å¯é€‰: ä» S3 ä¸‹è½½åˆ°æœ¬åœ°å¤„ç†

    Args:
        s3_path: S3 URI (s3://bucket/prefix/)
        local_path: æœ¬åœ°ç›®æ ‡è·¯å¾„
    """
    try:
        import boto3
        s3 = boto3.client('s3')

        # è§£æ S3 URI
        bucket = s3_path.replace('s3://', '').split('/')[0]
        prefix = '/'.join(s3_path.replace('s3://', '').split('/')[1:])

        # åˆ—å‡ºå¯¹è±¡
        paginator = s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get('Contents', []):
                key = obj['Key']
                if key.endswith('.json'):
                    local_file = Path(local_path) / Path(key).name
                    local_file.parent.mkdir(parents=True, exist_ok=True)
                    s3.download_file(bucket, key, str(local_file))
                    print(f"Downloaded: {key}")

    except ImportError:
        print("boto3 not installed. Cannot download from S3.")
    except Exception as e:
        print(f"Error downloading from S3: {e}")


# ==============================================================================
# Data Cleansing Functions (æ›¿ä»£ PySpark cleanse_data)
# ==============================================================================
def cleanse_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ•°æ®æ¸…æ´— (çº¯ Pandas å®ç°)

    Rules:
    1. ç§»é™¤ç©ºå€¼
    2. æ ‡å‡†åŒ–ç¬¦å·æ ¼å¼
    3. å¤„ç†è´Ÿä»·æ ¼
    4. å¡«å……ç¼ºå¤±å€¼
    """
    print("ğŸ§¹ Applying data cleansing rules...")

    initial_count = len(df)

    # 1. ç§»é™¤å¿…éœ€å­—æ®µçš„ç©ºå€¼
    df = df.dropna(subset=['symbol', 'timestamp'])

    # 2. æ ‡å‡†åŒ– symbol (å¤§å†™)
    df['symbol'] = df['symbol'].astype(str).str.upper().str.strip()

    # 3. å¤„ç†ä»·æ ¼åˆ—
    price_columns = ['open_price', 'high_price', 'low_price', 'close_price', 'adjusted_close']
    for col in price_columns:
        if col in df.columns:
            # æ›¿æ¢è´Ÿå€¼ä¸º NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            df[col] = df[col].where(df[col] >= 0, np.nan)

    # 4. å¡«å……ç¼ºå¤±å€¼
    df['volume'] = pd.to_numeric(df.get('volume', 0), errors='coerce').fillna(0)
    df['dividend_amount'] = pd.to_numeric(df.get('dividend_amount', 0), errors='coerce').fillna(0.0)
    df['split_coefficient'] = pd.to_numeric(df.get('split_coefficient', 1.0), errors='coerce').fillna(1.0)

    # 5. éªŒè¯ OHLC å®Œæ•´æ€§: high >= low
    df = df[
        (df['high_price'].isna()) |
        (df['low_price'].isna()) |
        (df['high_price'] >= df['low_price'])
    ]

    # 6. ç§»é™¤é‡å¤è®°å½• (symbol + timestamp)
    df = df.drop_duplicates(subset=['symbol', 'timestamp'], keep='first')

    final_count = len(df)
    removed = initial_count - final_count

    print(f"âœ… Cleansed: {initial_count} â†’ {final_count} records ({removed} removed)")

    return df


def standardize_dates(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ (æ›¿ä»£ PySpark to_date/to_timestamp)
    """
    print("ğŸ“… Standardizing dates...")

    # è½¬æ¢æ—¶é—´æˆ³
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df['trade_date'] = df['timestamp'].dt.date
    df['trade_timestamp'] = df['timestamp']

    # ç§»é™¤æ— æ•ˆæ—¥æœŸ
    df = df.dropna(subset=['trade_date'])

    return df


# ==============================================================================
# Technical Indicator Functions (çº¯ Pandas å®ç°)
# ==============================================================================
def calculate_derived_metrics(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—è¡ç”ŸæŒ‡æ ‡ (æ›¿ä»£ PySpark Window å‡½æ•°)

    ä½¿ç”¨ groupby + shift å®ç°
    """
    print("ğŸ“Š Calculating derived metrics...")

    # ç¡®ä¿æŒ‰ symbol å’Œ date æ’åº
    df = df.sort_values(['symbol', 'trade_date']).reset_index(drop=True)

    # 1. æ—¥å†…æ³¢åŠ¨
    df['daily_range'] = df['high_price'] - df['low_price']
    df['daily_range_pct'] = (df['daily_range'] / df['close_price']) * 100

    # 2. æ—¥æ”¶ç›Šç‡ (close-to-close)
    df['prev_close'] = df.groupby('symbol')['close_price'].shift(1)
    df['daily_return'] = ((df['close_price'] - df['prev_close']) / df['prev_close']) * 100

    # 3. æˆäº¤é‡å˜åŒ–
    df['prev_volume'] = df.groupby('symbol')['volume'].shift(1)
    df['volume_change_pct'] = ((df['volume'] - df['prev_volume']) / df['prev_volume']) * 100

    # æ¸…ç†ä¸´æ—¶åˆ—
    df = df.drop(columns=['prev_close', 'prev_volume'], errors='ignore')

    return df


def calculate_moving_averages(df: pd.DataFrame, periods: List[int] = None) -> pd.DataFrame:
    """
    è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡ (SMA)

    ä½¿ç”¨ pandas rolling å®ç°
    """
    print(f"ğŸ“ˆ Calculating SMA for periods: {periods or Config.SMA_PERIODS}...")

    periods = periods or Config.SMA_PERIODS

    for period in periods:
        df[f'sma_{period}'] = (
            df.groupby('symbol')['close_price']
            .transform(lambda x: x.rolling(window=period, min_periods=1).mean())
            .round(4)
        )

    return df


def calculate_ema(df: pd.DataFrame, periods: List[int] = None) -> pd.DataFrame:
    """
    è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA)

    ä½¿ç”¨ pandas ewm å®ç° (æ¯” PySpark æ›´ç²¾ç¡®)
    """
    print(f"ğŸ“‰ Calculating EMA for periods: {periods or Config.EMA_PERIODS}...")

    periods = periods or Config.EMA_PERIODS

    for period in periods:
        df[f'ema_{period}'] = (
            df.groupby('symbol')['close_price']
            .transform(lambda x: x.ewm(span=period, adjust=False).mean())
            .round(4)
        )

    return df


def calculate_volatility(df: pd.DataFrame, window: int = None) -> pd.DataFrame:
    """
    è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡ (æ ‡å‡†å·®)

    ä½¿ç”¨ pandas rolling std
    """
    window = window or Config.VOLATILITY_WINDOW
    print(f"ğŸ“Š Calculating {window}d volatility...")

    df['volatility_20d'] = (
        df.groupby('symbol')['daily_return']
        .transform(lambda x: x.rolling(window=window, min_periods=1).std())
        .round(4)
    )

    return df


def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ·»åŠ æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
    """
    df = calculate_derived_metrics(df)
    df = calculate_moving_averages(df)
    df = calculate_ema(df)
    df = calculate_volatility(df)

    return df


# ==============================================================================
# Dimensional Modeling (æ›¿ä»£ PySpark date functions)
# ==============================================================================
def add_date_dimensions(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ·»åŠ æ—¥æœŸç»´åº¦åˆ—
    """
    print("ğŸ“… Adding date dimensions...")

    df['year'] = pd.to_datetime(df['trade_date']).dt.year
    df['quarter'] = pd.to_datetime(df['trade_date']).dt.quarter
    df['month'] = pd.to_datetime(df['trade_date']).dt.month
    df['day'] = pd.to_datetime(df['trade_date']).dt.day
    df['day_of_week'] = pd.to_datetime(df['trade_date']).dt.dayofweek + 1  # 1=Monday

    return df


def add_metadata(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ·»åŠ å…ƒæ•°æ®
    """
    df['processing_timestamp'] = pd.Timestamp.now()
    df['price_id'] = (
        df['symbol'] + '_' +
        pd.to_datetime(df['trade_date']).dt.strftime('%Y%m%d')
    )

    return df


def select_final_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    é€‰æ‹©æœ€ç»ˆåˆ—
    """
    final_columns = [
        'price_id',
        'symbol',
        'trade_date',
        'trade_timestamp',
        'open_price',
        'high_price',
        'low_price',
        'close_price',
        'adjusted_close',
        'volume',
        'dividend_amount',
        'split_coefficient',
        'daily_return',
        'daily_range',
        'daily_range_pct',
        'volume_change_pct',
        'sma_5',
        'sma_20',
        'sma_50',
        'ema_12',
        'ema_26',
        'volatility_20d',
        'year',
        'quarter',
        'month',
        'day',
        'day_of_week',
        'processing_timestamp',
    ]

    # åªé€‰æ‹©å­˜åœ¨çš„åˆ—
    available_columns = [c for c in final_columns if c in df.columns]
    return df[available_columns]


# ==============================================================================
# Data Writing Functions (Parquet)
# ==============================================================================
def write_to_parquet(
    df: pd.DataFrame,
    output_path: str,
    partition_cols: List[str] = None,
    compression: str = 'snappy'
):
    """
    å†™å…¥ Parquet æ–‡ä»¶ (æ›¿ä»£ Spark write.parquet)

    ä½¿ç”¨ PyArrow å®ç°é«˜æ•ˆå‹ç¼©
    """
    print(f"ğŸ’¾ Writing to Parquet: {output_path}")

    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯æ­£ç¡®ç±»å‹
    if 'trade_date' in df.columns:
        df['trade_date'] = pd.to_datetime(df['trade_date'])

    if partition_cols:
        # åˆ†åŒºå†™å…¥ (æ¨¡æ‹Ÿ Spark partitionBy)
        print(f"Partitioning by: {partition_cols}")

        for partition_values, group_df in df.groupby(partition_cols):
            if not isinstance(partition_values, tuple):
                partition_values = (partition_values,)

            # æ„å»ºåˆ†åŒºè·¯å¾„
            partition_path = output_path
            for col, val in zip(partition_cols, partition_values):
                partition_path = partition_path / f"{col}={val}"

            partition_path.mkdir(parents=True, exist_ok=True)
            file_path = partition_path / "data.parquet"

            # å†™å…¥åˆ†åŒºæ–‡ä»¶
            group_df.to_parquet(
                file_path,
                compression=compression,
                index=False,
                engine='pyarrow'
            )

        print(f"âœ… Written {len(df)} records to {output_path} (partitioned)")

    else:
        # å•æ–‡ä»¶å†™å…¥
        df.to_parquet(
            output_path,
            compression=compression,
            index=False,
            engine='pyarrow'
        )

        print(f"âœ… Written {len(df)} records to {output_path}")

    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
    if output_path.is_file():
        size_mb = output_path.stat().st_size / (1024 * 1024)
        print(f"ğŸ“¦ File size: {size_mb:.2f} MB")


def write_to_csv(df: pd.DataFrame, output_path: str):
    """
    å¯é€‰: å†™å…¥ CSV (ä¾¿äºè°ƒè¯•)
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    df.to_csv(output_path, index=False)
    print(f"âœ… Written CSV to {output_path}")


# ==============================================================================
# DuckDB Query Examples (å¯é€‰)
# ==============================================================================
def query_with_duckdb(parquet_path: str, query: str = None):
    """
    ä½¿ç”¨ DuckDB ç›´æ¥æŸ¥è¯¢ Parquet æ–‡ä»¶

    DuckDB æ˜¯é›¶é…ç½®çš„åˆ†ææ•°æ®åº“ï¼Œå®Œå…¨å…è´¹
    """
    if not DUCKDB_AVAILABLE:
        print("DuckDB not available")
        return None

    conn = duckdb.connect(':memory:')

    default_query = f"""
    SELECT
        symbol,
        DATE_TRUNC('day', trade_date) as date,
        AVG(close_price) as avg_close,
        SUM(volume) as total_volume,
        COUNT(*) as record_count
    FROM read_parquet('{parquet_path}')
    GROUP BY symbol, date
    ORDER BY date DESC
    LIMIT 10
    """

    query = query or default_query

    result = conn.execute(query).df()
    print("\nğŸ“Š Query Result:")
    print(result)

    return result


# ==============================================================================
# Main ETL Pipeline
# ==============================================================================
def run_transformation(
    source_path: str,
    target_path: str,
    partition_by: List[str] = None,
    write_csv: bool = False
):
    """
    æ‰§è¡Œå®Œæ•´ ETL æµç¨‹ (çº¯ Pandasï¼Œé›¶ Spark ä¾èµ–)
    """
    print("=" * 70)
    print("ğŸš€ STARTING PANDAS-BASED FINANCIAL DATA TRANSFORMATION")
    print("=" * 70)
    print(f"Source: {source_path}")
    print(f"Target: {target_path}")
    print(f"Partition by: {partition_by or 'None'}")
    print("=" * 70)

    start_time = datetime.now()

    try:
        # Step 1: è¯»å–åŸå§‹æ•°æ®
        raw_df = read_json_files(source_path)

        # Step 2: æ•°æ®æ¸…æ´—
        cleansed_df = cleanse_data(raw_df)

        # Step 3: æ ‡å‡†åŒ–æ—¥æœŸ
        dated_df = standardize_dates(cleansed_df)

        # Step 4: æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        enriched_df = add_technical_indicators(dated_df)

        # Step 5: æ·»åŠ æ—¥æœŸç»´åº¦
        dimensional_df = add_date_dimensions(enriched_df)

        # Step 6: æ·»åŠ å…ƒæ•°æ®
        final_df = add_metadata(dimensional_df)

        # Step 7: é€‰æ‹©æœ€ç»ˆåˆ—
        output_df = select_final_columns(final_df)

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 70)
        print("ğŸ“Š TRANSFORMATION SUMMARY")
        print("=" * 70)
        print(f"Total records: {len(output_df)}")
        print(f"Symbols: {output_df['symbol'].nunique()}")
        print(f"Date range: {output_df['trade_date'].min()} to {output_df['trade_date'].max()}")
        print("\nSample data:")
        print(output_df.head(3).to_string())

        # Step 8: å†™å…¥ Parquet
        write_to_parquet(
            output_df,
            target_path,
            partition_cols=partition_by,
            compression=Config.PARQUET_COMPRESSION
        )

        # Step 9: å¯é€‰å†™å…¥ CSV
        if write_csv:
            csv_path = Path(target_path).with_suffix('.csv')
            write_to_csv(output_df, str(csv_path))

        # æ‰§è¡Œæ—¶é—´
        elapsed = datetime.now() - start_time
        print("\n" + "=" * 70)
        print(f"âœ… TRANSFORMATION COMPLETE in {elapsed.total_seconds():.2f} seconds")
        print("=" * 70)

        # å¯é€‰: DuckDB æŸ¥è¯¢ç¤ºä¾‹
        if DUCKDB_AVAILABLE and Path(target_path).is_file():
            print("\nğŸ” Running sample DuckDB query...")
            query_with_duckdb(target_path)

        return output_df

    except Exception as e:
        print(f"\nâŒ ERROR: Transformation failed - {e}")
        import traceback
        traceback.print_exc()
        raise


# ==============================================================================
# CLI Entry Point
# ==============================================================================
def main():
    """
    å‘½ä»¤è¡Œå…¥å£
    """
    parser = argparse.ArgumentParser(
        description='Local Pandas ETL Transform (Zero Spark Dependencies)'
    )

    parser.add_argument(
        '--source_path',
        type=str,
        required=True,
        help='Path to source JSON files (local directory or S3 URI)'
    )

    parser.add_argument(
        '--target_path',
        type=str,
        required=True,
        help='Output Parquet file path'
    )

    parser.add_argument(
        '--partition_by',
        type=str,
        nargs='+',
        default=None,
        help='Partition columns (e.g., year month)'
    )

    parser.add_argument(
        '--write_csv',
        action='store_true',
        help='Also write CSV output for debugging'
    )

    args = parser.parse_args()

    run_transformation(
        source_path=args.source_path,
        target_path=args.target_path,
        partition_by=args.partition_by,
        write_csv=args.write_csv
    )


if __name__ == "__main__":
    main()
