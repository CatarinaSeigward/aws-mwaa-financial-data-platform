#!/usr/bin/env python3
"""
Enhanced Data Generator for Financial Data Pipeline
===================================================
ç”Ÿæˆé«˜è´¨é‡ã€å¯é…ç½®çš„æ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®

Features:
- æ”¯æŒè‡ªå®šä¹‰è‚¡ç¥¨é…ç½®ï¼ˆä»·æ ¼ã€æ³¢åŠ¨ç‡ã€è¶‹åŠ¿ï¼‰
- æ¨¡æ‹ŸçœŸå®å¸‚åœºè¡Œä¸ºï¼ˆä»·æ ¼èµ°åŠ¿ã€æˆäº¤é‡ã€åˆ†çº¢ã€æ‹†è‚¡ï¼‰
- æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆJSONã€CSVã€Parquetï¼‰
- å¯ç”Ÿæˆé¢„å®šä¹‰æ•°æ®é›†ï¼ˆå°ã€ä¸­ã€å¤§è§„æ¨¡ï¼‰
- ä¸ Alpha Vantage API æ ¼å¼å®Œå…¨å…¼å®¹
- æ”¯æŒå¢é‡æ•°æ®ç”Ÿæˆ

Usage:
    # å¿«é€Ÿç”Ÿæˆé»˜è®¤æ•°æ®é›†
    python scripts/data_generator.py --preset demo

    # è‡ªå®šä¹‰ç”Ÿæˆ
    python scripts/data_generator.py \
        --symbols AAPL GOOGL MSFT \
        --days 180 \
        --format json \
        --trend bullish

    # ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
    python scripts/data_generator.py --preset large

    # å¢é‡ç”Ÿæˆï¼ˆè¿½åŠ æ–°æ•°æ®ï¼‰
    python scripts/data_generator.py \
        --incremental \
        --days 30
"""

import json
import csv
import argparse
import random
import logging
from pathlib import Path
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Tuple, Optional
import math

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
random.seed(42)


# ==============================================================================
# è‚¡ç¥¨é…ç½®åº“
# ==============================================================================

class StockConfig:
    """è‚¡ç¥¨é…ç½®ç±»"""

    CONFIGS = {
        "AAPL": {
            "name": "Apple Inc.",
            "sector": "Technology",
            "exchange": "NASDAQ",
            "initial_price": 180.0,
            "volatility": 0.020,      # 2% æ—¥æ³¢åŠ¨ç‡
            "trend": 0.0003,          # 0.03% æ—¥è¶‹åŠ¿
            "dividend_yield": 0.005,  # 0.5% å¹´åŒ–è‚¡æ¯
            "split_probability": 0.001,
            "volume_base": 60_000_000,
        },
        "GOOGL": {
            "name": "Alphabet Inc.",
            "sector": "Technology",
            "exchange": "NASDAQ",
            "initial_price": 140.0,
            "volatility": 0.025,
            "trend": 0.0004,
            "dividend_yield": 0.0,
            "split_probability": 0.0005,
            "volume_base": 25_000_000,
        },
        "MSFT": {
            "name": "Microsoft Corporation",
            "sector": "Technology",
            "exchange": "NASDAQ",
            "initial_price": 370.0,
            "volatility": 0.018,
            "trend": 0.0005,
            "dividend_yield": 0.008,
            "split_probability": 0.0003,
            "volume_base": 30_000_000,
        },
        "AMZN": {
            "name": "Amazon.com Inc.",
            "sector": "Consumer Cyclical",
            "exchange": "NASDAQ",
            "initial_price": 155.0,
            "volatility": 0.030,
            "trend": 0.0002,
            "dividend_yield": 0.0,
            "split_probability": 0.0008,
            "volume_base": 50_000_000,
        },
        "TSLA": {
            "name": "Tesla Inc.",
            "sector": "Automotive",
            "exchange": "NASDAQ",
            "initial_price": 240.0,
            "volatility": 0.040,
            "trend": 0.0001,
            "dividend_yield": 0.0,
            "split_probability": 0.001,
            "volume_base": 100_000_000,
        },
        "NVDA": {
            "name": "NVIDIA Corporation",
            "sector": "Technology",
            "exchange": "NASDAQ",
            "initial_price": 500.0,
            "volatility": 0.035,
            "trend": 0.0008,
            "dividend_yield": 0.001,
            "split_probability": 0.002,
            "volume_base": 40_000_000,
        },
        "META": {
            "name": "Meta Platforms Inc.",
            "sector": "Technology",
            "exchange": "NASDAQ",
            "initial_price": 350.0,
            "volatility": 0.028,
            "trend": 0.0003,
            "dividend_yield": 0.0,
            "split_probability": 0.0,
            "volume_base": 20_000_000,
        },
        "JPM": {
            "name": "JPMorgan Chase & Co.",
            "sector": "Financial Services",
            "exchange": "NYSE",
            "initial_price": 150.0,
            "volatility": 0.015,
            "trend": 0.0002,
            "dividend_yield": 0.025,
            "split_probability": 0.0001,
            "volume_base": 15_000_000,
        },
    }

    @classmethod
    def get(cls, symbol: str) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨é…ç½®"""
        return cls.CONFIGS.get(symbol.upper(), cls.CONFIGS["AAPL"])

    @classmethod
    def all_symbols(cls) -> List[str]:
        """è·å–æ‰€æœ‰æ”¯æŒçš„è‚¡ç¥¨ä»£ç """
        return list(cls.CONFIGS.keys())

    @classmethod
    def apply_market_condition(cls, symbol: str, condition: str) -> Dict[str, Any]:
        """
        æ ¹æ®å¸‚åœºæ¡ä»¶è°ƒæ•´é…ç½®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            condition: å¸‚åœºæ¡ä»¶ (bullish, bearish, volatile, stable)
        """
        config = cls.get(symbol).copy()

        if condition == "bullish":
            config["trend"] *= 3  # å¼ºåŠ¿ä¸Šæ¶¨
            config["volatility"] *= 0.8  # é™ä½æ³¢åŠ¨
        elif condition == "bearish":
            config["trend"] *= -2  # ä¸‹è·Œ
            config["volatility"] *= 1.2  # å¢åŠ æ³¢åŠ¨
        elif condition == "volatile":
            config["volatility"] *= 2  # é«˜æ³¢åŠ¨
            config["trend"] *= 0.5  # è¶‹åŠ¿å‡å¼±
        elif condition == "stable":
            config["volatility"] *= 0.5  # ä½æ³¢åŠ¨
            config["trend"] *= 0.5  # å¹³ç¨³

        return config


# ==============================================================================
# æ•°æ®ç”Ÿæˆå¼•æ“
# ==============================================================================

class StockDataGenerator:
    """è‚¡ç¥¨æ•°æ®ç”Ÿæˆå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.current_price = config["initial_price"]
        self.cumulative_dividend = 0.0
        self.split_coefficient = 1.0

    def generate_daily_ohlcv(self) -> Tuple[float, float, float, float, int]:
        """
        ç”Ÿæˆå•æ—¥ OHLCV æ•°æ®

        Returns:
            (open, high, low, close, volume)
        """
        volatility = self.config["volatility"]
        trend = self.config["trend"]
        volume_base = self.config["volume_base"]

        # å¼€ç›˜ä»·ï¼šå‰æ”¶ç›˜ Â± è·³ç©º
        gap = random.gauss(0, volatility / 2)
        open_price = self.current_price * (1 + gap)

        # æ—¥å†…æ³¢åŠ¨
        intraday_vol = abs(random.gauss(volatility, volatility / 3))
        intraday_vol = max(0.001, intraday_vol)

        # æ”¶ç›˜ä»·ï¼šè¶‹åŠ¿ + éšæœº
        price_change = random.gauss(trend, volatility)
        close_price = open_price * (1 + price_change)

        # é«˜ä½ä»·
        intraday_range = abs(random.gauss(0, intraday_vol))
        high_price = max(open_price, close_price) * (1 + intraday_range)
        low_price = min(open_price, close_price) * (1 - intraday_range)

        # ç¡®ä¿é€»è¾‘æ­£ç¡®
        high_price = max(high_price, open_price, close_price)
        low_price = min(low_price, open_price, close_price)

        # æˆäº¤é‡
        volume_multiplier = random.lognormvariate(0, 0.5)
        volume = int(volume_base * volume_multiplier)

        return (
            round(open_price, 4),
            round(high_price, 4),
            round(low_price, 4),
            round(close_price, 4),
            volume,
        )

    def generate_dividend(self, current_date: date, close_price: float) -> float:
        """
        ç”Ÿæˆåˆ†çº¢ï¼ˆå­£åº¦æœ«æ¦‚ç‡æ€§å‘æ”¾ï¼‰

        Args:
            current_date: å½“å‰æ—¥æœŸ
            close_price: æ”¶ç›˜ä»·

        Returns:
            åˆ†çº¢é‡‘é¢
        """
        if current_date.month % 3 != 0 or current_date.day < 28:
            return 0.0

        if random.random() < 0.3:  # 30% æ¦‚ç‡å‘æ”¾
            dividend = close_price * self.config["dividend_yield"] / 4
            self.cumulative_dividend += dividend
            return round(dividend, 4)

        return 0.0

    def generate_split(self) -> float:
        """
        ç”Ÿæˆè‚¡ç¥¨æ‹†åˆ†ï¼ˆç½•è§äº‹ä»¶ï¼‰

        Returns:
            æ‹†åˆ†ç³»æ•° (1.0 = æ— æ‹†åˆ†)
        """
        if random.random() < self.config["split_probability"]:
            split = random.choice([2.0, 3.0, 0.5])  # 2:1, 3:1 æˆ– 1:2
            self.split_coefficient *= split
            return split
        return 1.0

    def generate_history(
        self,
        symbol: str,
        start_date: date,
        end_date: date,
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå®Œæ•´å†å²æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æ•°æ®è®°å½•åˆ—è¡¨ï¼ˆæœ€æ–°æ—¥æœŸåœ¨å‰ï¼‰
        """
        records = []
        current_date = start_date

        while current_date <= end_date:
            # è·³è¿‡å‘¨æœ«
            if current_date.weekday() >= 5:
                current_date += timedelta(days=1)
                continue

            # ç”Ÿæˆ OHLC
            open_p, high, low, close_p, volume = self.generate_daily_ohlcv()

            # åˆ†çº¢
            dividend = self.generate_dividend(current_date, close_p)

            # æ‹†è‚¡
            split = self.generate_split()
            if split != 1.0:
                open_p /= split
                high /= split
                low /= split
                close_p /= split
                volume = int(volume * split)

            # è°ƒæ•´åæ”¶ç›˜ä»·
            adjusted_close = close_p - dividend

            # è®°å½•
            record = {
                "symbol": symbol,
                "timestamp": current_date.strftime("%Y-%m-%d"),
                "open_price": round(open_p, 4),
                "high_price": round(high, 4),
                "low_price": round(low, 4),
                "close_price": round(close_p, 4),
                "adjusted_close": round(adjusted_close, 4),
                "volume": volume,
                "dividend_amount": round(dividend, 4),
                "split_coefficient": round(split, 2),
            }

            records.append(record)

            # æ›´æ–°å½“å‰ä»·æ ¼
            self.current_price = close_p

            current_date += timedelta(days=1)

        # æœ€æ–°æ—¥æœŸåœ¨å‰
        return list(reversed(records))


# ==============================================================================
# è¾“å‡ºæ ¼å¼åŒ–å™¨
# ==============================================================================

class OutputFormatter:
    """è¾“å‡ºæ ¼å¼åŒ–å™¨"""

    @staticmethod
    def to_json(
        records: List[Dict[str, Any]],
        symbol: str,
        output_file: Path,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """ä¿å­˜ä¸º JSONï¼ˆAlpha Vantage æ ¼å¼ï¼‰"""
        data = {
            "meta": metadata or {
                "symbol": symbol,
                "last_refreshed": datetime.now().strftime("%Y-%m-%d"),
                "output_size": "full",
                "time_zone": "US/Eastern",
                "ingestion_timestamp": datetime.utcnow().isoformat(),
                "data_source": "simulated",
            },
            "data": records,
        }

        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… JSON saved: {output_file} ({len(records)} records)")

    @staticmethod
    def to_csv(records: List[Dict[str, Any]], output_file: Path):
        """ä¿å­˜ä¸º CSV"""
        if not records:
            return

        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=records[0].keys())
            writer.writeheader()
            writer.writerows(records)

        logger.info(f"âœ… CSV saved: {output_file} ({len(records)} records)")

    @staticmethod
    def to_parquet(records: List[Dict[str, Any]], output_file: Path):
        """ä¿å­˜ä¸º Parquet"""
        try:
            import pandas as pd

            df = pd.DataFrame(records)
            df['timestamp'] = pd.to_datetime(df['timestamp'])

            output_file.parent.mkdir(parents=True, exist_ok=True)
            df.to_parquet(output_file, compression='snappy', index=False)

            logger.info(f"âœ… Parquet saved: {output_file} ({len(records)} records)")
        except ImportError:
            logger.error("âŒ pandas/pyarrow not installed. Cannot save Parquet.")


# ==============================================================================
# é¢„è®¾æ•°æ®é›†
# ==============================================================================

class Presets:
    """é¢„å®šä¹‰æ•°æ®é›†"""

    DEMO = {
        "name": "Demo Dataset",
        "symbols": ["AAPL", "GOOGL", "MSFT", "AMZN", "TSLA"],
        "days": 90,
        "market_condition": "normal",
        "description": "é€‚åˆå¿«é€Ÿæ¼”ç¤ºçš„å°è§„æ¨¡æ•°æ®é›†",
    }

    SMALL = {
        "name": "Small Dataset",
        "symbols": ["AAPL", "GOOGL", "MSFT"],
        "days": 30,
        "market_condition": "normal",
        "description": "å°è§„æ¨¡æµ‹è¯•æ•°æ®é›†",
    }

    MEDIUM = {
        "name": "Medium Dataset",
        "symbols": ["AAPL", "GOOGL", "MSFT", "AMZN", "TSLA", "NVDA"],
        "days": 180,
        "market_condition": "normal",
        "description": "ä¸­ç­‰è§„æ¨¡å¼€å‘æ•°æ®é›†",
    }

    LARGE = {
        "name": "Large Dataset",
        "symbols": StockConfig.all_symbols(),
        "days": 365,
        "market_condition": "normal",
        "description": "å¤§è§„æ¨¡å‹åŠ›æµ‹è¯•æ•°æ®é›†",
    }

    @classmethod
    def get(cls, preset: str) -> Dict[str, Any]:
        """è·å–é¢„è®¾é…ç½®"""
        presets = {
            "demo": cls.DEMO,
            "small": cls.SMALL,
            "medium": cls.MEDIUM,
            "large": cls.LARGE,
        }
        return presets.get(preset.lower(), cls.DEMO)


# ==============================================================================
# ä¸»ç”Ÿæˆå™¨
# ==============================================================================

class DataGeneratorOrchestrator:
    """æ•°æ®ç”Ÿæˆç¼–æ’å™¨"""

    def __init__(
        self,
        symbols: List[str],
        start_date: date,
        end_date: date,
        output_dir: Path,
        output_format: str = "json",
        market_condition: str = "normal",
        execution_date: Optional[str] = None,
    ):
        self.symbols = symbols
        self.start_date = start_date
        self.end_date = end_date
        self.output_dir = output_dir
        self.output_format = output_format
        self.market_condition = market_condition
        self.execution_date = execution_date or end_date.strftime("%Y-%m-%d")

    def generate_all(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰€æœ‰æ•°æ®"""
        logger.info("=" * 70)
        logger.info("ğŸš€ Starting Data Generation")
        logger.info("=" * 70)
        logger.info(f"ğŸ“… Date Range: {self.start_date} to {self.end_date}")
        logger.info(f"ğŸ“ˆ Symbols: {', '.join(self.symbols)}")
        logger.info(f"ğŸ“Š Market Condition: {self.market_condition}")
        logger.info(f"ğŸ’¾ Output: {self.output_dir}")
        logger.info(f"ğŸ“„ Format: {self.output_format}")
        logger.info("=" * 70)

        results = {
            "symbols": {},
            "total_records": 0,
            "output_files": [],
        }

        for symbol in self.symbols:
            logger.info(f"\nğŸ“Š Generating data for {symbol}...")

            # è·å–é…ç½®
            config = StockConfig.apply_market_condition(symbol, self.market_condition)

            # ç”Ÿæˆæ•°æ®
            generator = StockDataGenerator(config)
            records = generator.generate_history(symbol, self.start_date, self.end_date)

            # ä¿å­˜æ•°æ®
            output_file = self._save_data(symbol, records)

            results["symbols"][symbol] = {
                "records": len(records),
                "file": str(output_file),
            }
            results["total_records"] += len(records)
            results["output_files"].append(output_file)

            logger.info(f"   âœ… Generated {len(records):,} records")

        self._print_summary(results)

        return results

    def _save_data(self, symbol: str, records: List[Dict[str, Any]]) -> Path:
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        # åˆ›å»ºåˆ†åŒºç›®å½•
        partition_dir = self.output_dir / f"date={self.execution_date}" / f"symbol={symbol}"

        # æ ¹æ®æ ¼å¼ä¿å­˜
        if self.output_format == "json":
            output_file = partition_dir / f"{symbol}_{self.execution_date}.json"
            OutputFormatter.to_json(records, symbol, output_file)
        elif self.output_format == "csv":
            output_file = partition_dir / f"{symbol}_{self.execution_date}.csv"
            OutputFormatter.to_csv(records, output_file)
        elif self.output_format == "parquet":
            output_file = partition_dir / f"{symbol}_{self.execution_date}.parquet"
            OutputFormatter.to_parquet(records, output_file)
        else:
            raise ValueError(f"Unsupported format: {self.output_format}")

        return output_file

    def _print_summary(self, results: Dict[str, Any]):
        """æ‰“å°ç”Ÿæˆæ‘˜è¦"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š Generation Complete")
        logger.info("=" * 70)

        logger.info(f"\nğŸ“ˆ Records per Symbol:")
        for symbol, data in results["symbols"].items():
            logger.info(f"   {symbol}: {data['records']:,} records")

        logger.info(f"\nğŸ’¾ Output Files:")
        for file in results["output_files"][:5]:
            size_kb = file.stat().st_size / 1024
            logger.info(f"   {file.name} ({size_kb:.1f} KB)")

        if len(results["output_files"]) > 5:
            logger.info(f"   ... and {len(results['output_files']) - 5} more")

        total_size = sum(f.stat().st_size for f in results["output_files"]) / (1024 * 1024)
        logger.info(f"\nğŸ’¿ Total: {results['total_records']:,} records, {total_size:.2f} MB")
        logger.info("\nâœ… Ready for ETL pipeline!")
        logger.info("=" * 70)


# ==============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ==============================================================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Enhanced Financial Data Generator',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Quick demo dataset
  python scripts/data_generator.py --preset demo

  # Custom generation
  python scripts/data_generator.py --symbols AAPL GOOGL --days 180

  # Bullish market scenario
  python scripts/data_generator.py --preset medium --market bullish

  # Large-scale testing
  python scripts/data_generator.py --preset large --format parquet

Presets:
  demo   - 5 symbols, 90 days (recommended for interviews)
  small  - 3 symbols, 30 days
  medium - 6 symbols, 180 days
  large  - 8 symbols, 365 days

Market Conditions:
  normal   - Standard market behavior
  bullish  - Strong uptrend with lower volatility
  bearish  - Downtrend with higher volatility
  volatile - High volatility with weak trend
  stable   - Low volatility with weak trend
        """
    )

    # é¢„è®¾æˆ–è‡ªå®šä¹‰
    parser.add_argument(
        '--preset',
        choices=['demo', 'small', 'medium', 'large'],
        help='Use predefined dataset configuration'
    )

    # è‡ªå®šä¹‰å‚æ•°
    parser.add_argument(
        '--symbols',
        nargs='+',
        choices=StockConfig.all_symbols(),
        help='Stock symbols to generate'
    )

    parser.add_argument(
        '--days',
        type=int,
        help='Number of days to generate'
    )

    parser.add_argument(
        '--start-date',
        type=str,
        help='Start date (YYYY-MM-DD)'
    )

    parser.add_argument(
        '--end-date',
        type=str,
        help='End date (YYYY-MM-DD)'
    )

    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        '--output',
        type=str,
        default='data/raw',
        help='Output directory (default: data/raw)'
    )

    parser.add_argument(
        '--format',
        choices=['json', 'csv', 'parquet'],
        default='json',
        help='Output format (default: json)'
    )

    parser.add_argument(
        '--execution-date',
        type=str,
        help='Execution date for partitioning (default: end-date)'
    )

    # å¸‚åœºæ¡ä»¶
    parser.add_argument(
        '--market',
        choices=['normal', 'bullish', 'bearish', 'volatile', 'stable'],
        default='normal',
        help='Market condition (default: normal)'
    )

    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for reproducibility (default: 42)'
    )

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)

    # ç¡®å®šé…ç½®
    if args.preset:
        preset_config = Presets.get(args.preset)
        symbols = preset_config["symbols"]
        days = preset_config["days"]
        market_condition = preset_config.get("market_condition", "normal")
        logger.info(f"ğŸ“¦ Using preset: {preset_config['name']}")
        logger.info(f"   {preset_config['description']}")
    else:
        symbols = args.symbols or ["AAPL", "GOOGL", "MSFT"]
        days = args.days or 90
        market_condition = args.market

    # ç¡®å®šæ—¥æœŸèŒƒå›´
    if args.end_date:
        end_date = datetime.strptime(args.end_date, '%Y-%m-%d').date()
    else:
        end_date = date.today()

    if args.start_date:
        start_date = datetime.strptime(args.start_date, '%Y-%m-%d').date()
    else:
        start_date = end_date - timedelta(days=days)

    # åˆ›å»ºç”Ÿæˆå™¨
    orchestrator = DataGeneratorOrchestrator(
        symbols=symbols,
        start_date=start_date,
        end_date=end_date,
        output_dir=Path(args.output),
        output_format=args.format,
        market_condition=market_condition,
        execution_date=args.execution_date,
    )

    # ç”Ÿæˆæ•°æ®
    orchestrator.generate_all()


if __name__ == "__main__":
    main()
