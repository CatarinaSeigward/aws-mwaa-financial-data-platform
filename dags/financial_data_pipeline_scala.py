"""
Financial Data Pipeline DAG - Scala + Spark Version
====================================================
Airflow DAG that orchestrates Scala Spark ETL jobs

Features:
- Scala + Spark data transformation (æ›¿ä»£ PySpark)
- å¯åœ¨æœ¬åœ° Docker Spark é›†ç¾¤è¿è¡Œ
- å¯æäº¤åˆ° AWS Glue (ç”Ÿäº§ç¯å¢ƒ)
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œé‡è¯•ç­–ç•¥

Author: Financial Data Platform Team
"""

import os
from datetime import datetime, timedelta
from pathlib import Path

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.task_group import TaskGroup

# Configuration
PROJECT_ROOT = Path(os.getenv('AIRFLOW_HOME', '/opt/airflow'))
SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')
JAR_PATH = PROJECT_ROOT / 'jars' / 'financial-etl-1.0.0.jar'
MAIN_CLASS = 'com.financial.etl.transform.FinancialDataTransform'

# Deployment mode
DEPLOYMENT_MODE = os.getenv('DEPLOYMENT_MODE', 'local')  # 'local' or 'aws'

# Data paths - dynamically switch between local and S3
if DEPLOYMENT_MODE == 'aws':
    RAW_DATA_PATH = os.getenv('S3_RAW_BUCKET', 's3://financial-raw/data/raw')
    CURATED_DATA_PATH = os.getenv('S3_CURATED_BUCKET', 's3://financial-processed/data/curated')
    VALIDATION_PATH = os.getenv('S3_VALIDATION_BUCKET', 's3://financial-validation/reports')
else:
    RAW_DATA_PATH = PROJECT_ROOT / 'data' / 'raw'
    CURATED_DATA_PATH = PROJECT_ROOT / 'data' / 'curated'
    VALIDATION_PATH = PROJECT_ROOT / 'data' / 'validation_reports'

# DAG configuration
DEFAULT_ARGS = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=2),
}

# SLA configuration
SLA = timedelta(hours=3)


def check_spark_cluster(**context):
    """
    éªŒè¯ Spark é›†ç¾¤æ˜¯å¦å¯ç”¨
    """
    import socket

    master_host = SPARK_MASTER.split('://')[1].split(':')[0]
    master_port = int(SPARK_MASTER.split(':')[-1])

    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        result = sock.connect_ex((master_host, master_port))
        sock.close()

        if result == 0:
            print(f"âœ… Spark cluster is reachable: {SPARK_MASTER}")
            return True
        else:
            raise ConnectionError(f"Cannot connect to Spark master: {SPARK_MASTER}")

    except Exception as e:
        print(f"âŒ Spark cluster check failed: {e}")
        raise


def verify_jar_exists(**context):
    """
    éªŒè¯ Scala JAR æ–‡ä»¶å­˜åœ¨
    """
    if not JAR_PATH.exists():
        raise FileNotFoundError(
            f"JAR file not found: {JAR_PATH}\n"
            f"Build it with: sbt assembly"
        )

    jar_size = JAR_PATH.stat().st_size / (1024 * 1024)
    print(f"âœ… JAR found: {JAR_PATH} ({jar_size:.2f} MB)")

    return str(JAR_PATH)


def upload_to_s3_if_needed(**context):
    """
    å¦‚æœæ˜¯ AWS éƒ¨ç½²æ¨¡å¼,ä¸Šä¼ ç”Ÿæˆçš„æ•°æ®åˆ° S3
    """
    if DEPLOYMENT_MODE != 'aws':
        print("âœ… Local mode - skip S3 upload")
        return

    import boto3
    from pathlib import Path

    print(f"â˜ï¸  Uploading data to S3...")

    s3_client = boto3.client('s3')
    local_data_dir = PROJECT_ROOT / 'data' / 'raw'

    # Extract bucket name from S3 URL
    bucket_name = RAW_DATA_PATH.replace('s3://', '').split('/')[0]
    s3_prefix = '/'.join(RAW_DATA_PATH.replace('s3://', '').split('/')[1:])

    uploaded_count = 0
    for file_path in local_data_dir.rglob('*.json'):
        s3_key = f"{s3_prefix}/{file_path.relative_to(local_data_dir)}"
        s3_client.upload_file(str(file_path), bucket_name, s3_key)
        uploaded_count += 1

    print(f"âœ… Uploaded {uploaded_count} files to {RAW_DATA_PATH}")


def generate_sample_data(**context):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®ï¼ˆé»˜è®¤æ–¹å¼ï¼‰

    ä¼˜åŠ¿ï¼š
    - æ— éœ€ API Key
    - æ— ç½‘ç»œä¾èµ–
    - æ— é€Ÿç‡é™åˆ¶
    - 100% å¯é 
    - é€‚åˆæ¼”ç¤ºå’Œæµ‹è¯•

    å¦‚éœ€ä½¿ç”¨çœŸå® APIï¼Œè¯·ä½¿ç”¨ fetch_stock_data_from_api() å‡½æ•°
    """
    import subprocess

    execution_date = context['ds']

    print(f"ğŸ² Generating sample data for {execution_date}...")
    print(f"   Deployment mode: {DEPLOYMENT_MODE}")
    print("   (Using simulated data for stable demo)")

    # ç”Ÿæˆåˆ°æœ¬åœ°ä¸´æ—¶ç›®å½•
    local_output = PROJECT_ROOT / 'data' / 'raw' if DEPLOYMENT_MODE == 'aws' else RAW_DATA_PATH

    # è°ƒç”¨æ•°æ®ç”Ÿæˆè„šæœ¬
    cmd = [
        'python',
        'scripts/data_generator.py',
        '--preset', 'demo',  # é»˜è®¤ä½¿ç”¨ demo é¢„è®¾
        '--execution-date', execution_date,
        '--output', str(local_output),
    ]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True,
        )

        print(result.stdout)

        # ç»Ÿè®¡ç”Ÿæˆçš„æ•°æ®
        output_dir = RAW_DATA_PATH / f"date={execution_date}"
        json_files = list(output_dir.glob('**/*.json'))

        total_records = 0
        for json_file in json_files:
            import json
            with open(json_file) as f:
                data = json.load(f)
                total_records += len(data.get('data', []))

        print(f"\nâœ… Generated {total_records} records from {len(json_files)} files")

        # Push to XCom
        context['task_instance'].xcom_push(key='total_records', value=total_records)
        context['task_instance'].xcom_push(key='output_path', value=str(output_dir))
        context['task_instance'].xcom_push(key='data_source', value='simulated')

    except subprocess.CalledProcessError as e:
        print(f"âŒ Error generating data: {e}")
        print(f"STDOUT: {e.stdout}")
        print(f"STDERR: {e.stderr}")
        raise


def fetch_stock_data_from_api(**context):
    """
    ä» Alpha Vantage API è·å–çœŸå®è‚¡ç¥¨æ•°æ®ï¼ˆå¤‡ç”¨æ–¹å¼ï¼‰

    ä½¿ç”¨åœºæ™¯ï¼š
    - éœ€è¦çœŸå®å†å²æ•°æ®
    - ç”Ÿäº§ç¯å¢ƒ
    - å›æµ‹éªŒè¯

    è¦æ±‚ï¼š
    - è®¾ç½® ALPHA_VANTAGE_API_KEY ç¯å¢ƒå˜é‡
    - æ³¨æ„ API é™é¢ï¼ˆ5 req/min, 500 req/dayï¼‰
    """
    import sys
    import json
    from pathlib import Path

    # Add src to path
    sys.path.insert(0, str(PROJECT_ROOT / 'src'))

    from ingestion.alpha_vantage_client import AlphaVantageClient

    # Initialize client
    api_key = os.getenv('ALPHA_VANTAGE_API_KEY')
    if not api_key:
        raise ValueError(
            "ALPHA_VANTAGE_API_KEY not set.\n"
            "Get free API key at: https://www.alphavantage.co/support/#api-key\n"
            "Or use generate_sample_data() for demo purposes."
        )

    client = AlphaVantageClient(api_key=api_key)

    # Symbols to fetch
    symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']

    execution_date = context['ds']
    output_dir = RAW_DATA_PATH / f"date={execution_date}"
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“¥ Fetching REAL data from Alpha Vantage API...")
    print(f"   Symbols: {', '.join(symbols)}")

    total_records = 0
    for symbol in symbols:
        try:
            print(f"  Fetching {symbol}...")
            data = client.get_daily_adjusted(symbol, outputsize='compact')

            if not data:
                print(f"  âš ï¸  No data for {symbol}")
                continue

            # Save to JSON
            symbol_dir = output_dir / f"symbol={symbol}"
            symbol_dir.mkdir(parents=True, exist_ok=True)

            output_file = symbol_dir / f"{symbol}_{execution_date}.json"
            with open(output_file, 'w') as f:
                json.dump(data, f, indent=2)

            records_count = len(data.get('data', []))
            total_records += records_count
            print(f"  âœ… {symbol}: {records_count} records")

        except Exception as e:
            print(f"  âŒ Error fetching {symbol}: {e}")
            continue

    if total_records == 0:
        raise ValueError("No data fetched from API")

    print(f"\nâœ… Total fetched: {total_records} records from Alpha Vantage API")

    # Push to XCom
    context['task_instance'].xcom_push(key='total_records', value=total_records)
    context['task_instance'].xcom_push(key='output_path', value=str(output_dir))
    context['task_instance'].xcom_push(key='data_source', value='alpha_vantage_api')


def verify_ingestion(**context):
    """
    éªŒè¯æ•°æ®æ‘„å–å®Œæˆ
    """
    execution_date = context['ds']
    output_dir = RAW_DATA_PATH / f"date={execution_date}"

    if not output_dir.exists():
        raise FileNotFoundError(f"Output directory not found: {output_dir}")

    json_files = list(output_dir.glob('**/*.json'))

    if not json_files:
        raise ValueError(f"No JSON files found in {output_dir}")

    total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)

    print(f"âœ… Ingestion verified:")
    print(f"   Files: {len(json_files)}")
    print(f"   Total size: {total_size:.2f} MB")
    print(f"   Location: {output_dir}")


def verify_transformation(**context):
    """
    éªŒè¯ Spark è½¬æ¢è¾“å‡º
    """
    execution_date = context['ds']

    # Check processed directory
    processed_dir = CURATED_DATA_PATH / 'processed'
    if not processed_dir.exists():
        raise FileNotFoundError(f"Processed directory not found: {processed_dir}")

    # Check for parquet files
    parquet_files = list(processed_dir.glob('**/*.parquet'))
    if not parquet_files:
        raise ValueError(f"No parquet files found in {processed_dir}")

    total_size = sum(f.stat().st_size for f in parquet_files) / (1024 * 1024)

    print(f"âœ… Transformation verified:")
    print(f"   Parquet files: {len(parquet_files)}")
    print(f"   Total size: {total_size:.2f} MB")

    # Check daily snapshot
    snapshot_path = CURATED_DATA_PATH / 'daily_snapshots' / f"date={execution_date}"
    if snapshot_path.exists():
        snapshot_files = list(snapshot_path.glob('*.parquet'))
        print(f"   Snapshot files: {len(snapshot_files)}")


def load_to_postgres(**context):
    """
    ä» Parquet åŠ è½½åˆ° PostgreSQL (æ›¿ä»£ Redshift)
    """
    import sys
    sys.path.insert(0, str(PROJECT_ROOT / 'src'))

    from loading.postgres_loader import load_parquet_to_postgres

    execution_date = context['ds']
    snapshot_path = CURATED_DATA_PATH / 'daily_snapshots' / f"date={execution_date}"

    # Find parquet file
    parquet_files = list(snapshot_path.glob('*.parquet'))
    if not parquet_files:
        raise FileNotFoundError(f"No parquet files in {snapshot_path}")

    parquet_file = parquet_files[0]

    print(f"ğŸ“¥ Loading from: {parquet_file}")

    # Load to PostgreSQL
    rows_loaded = load_parquet_to_postgres(
        parquet_path=str(parquet_file),
        table_name='fact_stock_prices',
        connection_params={
            'host': 'postgres',
            'database': 'financial_dw',
            'user': 'airflow',
            'password': 'airflow'
        }
    )

    print(f"âœ… Loaded {rows_loaded} rows to PostgreSQL")

    context['task_instance'].xcom_push(key='rows_loaded', value=rows_loaded)


def send_success_notification(**context):
    """
    å‘é€æˆåŠŸé€šçŸ¥
    """
    execution_date = context['ds']
    total_records = context['task_instance'].xcom_pull(
        task_ids='fetch_stock_data',
        key='total_records'
    )
    rows_loaded = context['task_instance'].xcom_pull(
        task_ids='load_to_postgres',
        key='rows_loaded'
    )

    message = f"""
âœ… Financial Data Pipeline Success

Execution Date: {execution_date}
Records Fetched: {total_records}
Records Loaded: {rows_loaded}

Scala + Spark transformation completed successfully!
    """

    print(message)

    # Optional: Send to Slack
    # slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
    # if slack_webhook:
    #     import requests
    #     requests.post(slack_webhook, json={'text': message})


# ============================================================================
# DAG Definition
# ============================================================================

with DAG(
    dag_id='financial_data_pipeline_scala',
    default_args=DEFAULT_ARGS,
    description='Financial data ETL pipeline with Scala + Spark',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['scala', 'spark', 'etl', 'financial'],
    max_active_runs=1,
    sla_miss_callback=None,  # Add custom SLA callback if needed
) as dag:

    # Start marker
    start = DummyOperator(task_id='start')

    # Pre-flight checks
    with TaskGroup('pre_flight_checks', tooltip='éªŒè¯ç¯å¢ƒé…ç½®') as pre_flight:

        check_spark = PythonOperator(
            task_id='check_spark_cluster',
            python_callable=check_spark_cluster,
            doc_md="""
            ### Check Spark Cluster
            éªŒè¯ Spark é›†ç¾¤æ˜¯å¦å¯ç”¨
            """
        )

        check_jar = PythonOperator(
            task_id='verify_jar_exists',
            python_callable=verify_jar_exists,
            doc_md="""
            ### Verify JAR
            éªŒè¯ Scala JAR æ–‡ä»¶å­˜åœ¨
            """
        )

        check_spark >> check_jar

    # Data ingestion - é»˜è®¤ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    fetch = PythonOperator(
        task_id='generate_sample_data',
        python_callable=generate_sample_data,  # æ”¹ä¸ºä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
        sla=timedelta(minutes=10),
        doc_md="""
        ### Generate Sample Data
        ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®ï¼ˆæ¨èç”¨äºæ¼”ç¤ºï¼‰

        - Symbols: AAPL, GOOGL, MSFT, AMZN, TSLA
        - Data: æ¨¡æ‹ŸçœŸå®å¸‚åœºè¡Œä¸ºï¼ˆä»·æ ¼æ³¢åŠ¨ã€åˆ†çº¢ã€æ‹†è‚¡ï¼‰
        - Output: JSON files in data/raw/
        - ä¼˜åŠ¿: æ— éœ€ API Key, 100% ç¨³å®š, æ— é€Ÿç‡é™åˆ¶

        å¦‚éœ€ä½¿ç”¨çœŸå® API æ•°æ®ï¼Œè¯·æ›¿æ¢ä¸º fetch_stock_data_from_api
        """
    )

    # Upload to S3 if AWS mode
    upload_s3 = PythonOperator(
        task_id='upload_to_s3',
        python_callable=upload_to_s3_if_needed,
        doc_md="""
        ### Upload to S3 (AWS mode only)
        å¦‚æœ DEPLOYMENT_MODE=awsï¼Œä¸Šä¼ æ•°æ®åˆ° S3
        """
    )

    verify_fetch = PythonOperator(
        task_id='verify_ingestion',
        python_callable=verify_ingestion,
        doc_md="""
        ### Verify Ingestion
        éªŒè¯æ•°æ®æ‘„å–å®Œæˆ
        """
    )

    # Scala Spark transformation
    spark_transform = SparkSubmitOperator(
        task_id='scala_spark_transform',
        application=str(JAR_PATH),
        java_class=MAIN_CLASS,
        conn_id='spark_default',
        conf={
            'spark.sql.shuffle.partitions': '8',
            'spark.sql.adaptive.enabled': 'true',
            'spark.sql.adaptive.coalescePartitions.enabled': 'true',
        },
        driver_memory='1g',
        executor_memory='2g',
        executor_cores=2,
        num_executors=2,
        application_args=[
            '--source-path', str(RAW_DATA_PATH),
            '--target-path', str(CURATED_DATA_PATH),
            '--execution-date', '{{ ds }}',
        ],
        sla=timedelta(hours=1),
        doc_md="""
        ### Scala Spark Transformation

        è¿è¡Œ Scala + Spark ETL ä½œä¸š:
        - è¯»å– JSON åŸå§‹æ•°æ®
        - æ•°æ®æ¸…æ´—å’ŒéªŒè¯
        - è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (SMA, EMA, æ³¢åŠ¨ç‡)
        - å†™å…¥ Parquet æ–‡ä»¶

        **Language:** Scala 2.12
        **Framework:** Apache Spark 3.4.1
        """
    )

    verify_transform = PythonOperator(
        task_id='verify_transformation',
        python_callable=verify_transformation,
        doc_md="""
        ### Verify Transformation
        éªŒè¯ Spark è½¬æ¢è¾“å‡º
        """
    )

    # Load to data warehouse
    load = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres,
        sla=timedelta(minutes=30),
        doc_md="""
        ### Load to PostgreSQL
        å°† Parquet æ•°æ®åŠ è½½åˆ° PostgreSQL
        """
    )

    # Success notification
    notify = PythonOperator(
        task_id='send_success_notification',
        python_callable=send_success_notification,
        trigger_rule='all_success',
        doc_md="""
        ### Success Notification
        å‘é€æˆåŠŸé€šçŸ¥
        """
    )

    # End marker
    end = DummyOperator(task_id='end')

    # Define task dependencies
    start >> pre_flight >> fetch >> upload_s3 >> verify_fetch
    verify_fetch >> spark_transform >> verify_transform
    verify_transform >> load >> notify >> end


# ============================================================================
# DAG Documentation
# ============================================================================

dag.doc_md = """
# Financial Data Pipeline - Scala + Spark Version

## æ¦‚è¿°

ä½¿ç”¨ **Scala + Spark** çš„ç”Ÿäº§çº§é‡‘èæ•°æ® ETL ç®¡é“ã€‚

## æ¶æ„

```
Alpha Vantage API
      â†“
  [Python] fetch_stock_data
      â†“
  JSON â†’ data/raw/
      â†“
  [Scala + Spark] spark_transform
      â†“
  Parquet â†’ data/curated/
      â†“
  [Python] load_to_postgres
      â†“
  PostgreSQL
```

## æŠ€æœ¯æ ˆ

- **ç¼–æ’**: Apache Airflow
- **æ•°æ®å¤„ç†**: Scala + Apache Spark 3.4.1
- **å­˜å‚¨æ ¼å¼**: Parquet (Snappy å‹ç¼©)
- **æ•°æ®ä»“åº“**: PostgreSQL (æœ¬åœ°) / Redshift (ç”Ÿäº§)

## æ ¸å¿ƒåŠŸèƒ½

### Scala Spark è½¬æ¢
- âœ… ç±»å‹å®‰å…¨çš„æ•°æ®å¤„ç†
- âœ… é«˜æ€§èƒ½å¹¶è¡Œè®¡ç®—
- âœ… å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
- âœ… ä¼˜åŒ–çš„ Parquet å†™å…¥

### æŠ€æœ¯æŒ‡æ ‡
- Simple Moving Averages (SMA): 5, 20, 50 å¤©
- Exponential Moving Averages (EMA): 12, 26 å¤©
- æ³¢åŠ¨ç‡ (Volatility): 20 å¤©æ ‡å‡†å·®
- æ—¥æ”¶ç›Šç‡ã€ä»·æ ¼åŒºé—´ã€æˆäº¤é‡å˜åŒ–

## è¿è¡Œç¯å¢ƒ

### æœ¬åœ° Docker
```bash
# å¯åŠ¨ Spark é›†ç¾¤
docker-compose -f docker-compose-spark.yml up -d

# æ„å»º Scala JAR
sbt assembly

# è§¦å‘ DAG
airflow dags trigger financial_data_pipeline_scala
```

### AWS ç”Ÿäº§ç¯å¢ƒ
- æ›¿æ¢ PostgreSQL â†’ Redshift
- æ›¿æ¢æœ¬åœ° Spark â†’ AWS Glue
- æ·»åŠ  S3 ä½œä¸ºæ•°æ®å­˜å‚¨

## ç›‘æ§

- Airflow UI: http://localhost:8080
- Spark Master UI: http://localhost:8081
- Spark Application UI: http://localhost:4040

## æˆæœ¬ä¼˜åŒ–

**æœ¬åœ°å¼€å‘**: $0/æœˆ
- Docker Compose è¿è¡Œå…¨éƒ¨æœåŠ¡
- å®Œå…¨å…è´¹

**AWS ç”Ÿäº§**: å¯é€šè¿‡ä»¥ä¸‹ä¼˜åŒ–é™ä½æˆæœ¬
- ä½¿ç”¨ Glue Flex æ‰§è¡Œç±»
- Redshift è‡ªåŠ¨æš‚åœ
- S3 ç”Ÿå‘½å‘¨æœŸç­–ç•¥

## ç»´æŠ¤

- å®šæœŸæ£€æŸ¥ Spark æ—¥å¿—
- ç›‘æ§ Parquet æ–‡ä»¶å¤§å°
- éªŒè¯æ•°æ®è´¨é‡æŒ‡æ ‡
"""
